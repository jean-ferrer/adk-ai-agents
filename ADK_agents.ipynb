{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e396bcaa",
   "metadata": {},
   "source": [
    "# ADK Gov AI Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddc8d2",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.5em; line-height: 1.3;\">\n",
    "Automação de Workflow de Data com Dados Públicos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1814f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ba55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jean\\Desktop\\ADK Gov AI Agents\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:198: UserWarning: Field name \"config_type\" in \"SequentialAgent\" shadows an attribute in parent \"BaseAgent\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from google.adk.agents import LlmAgent, LoopAgent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from google.genai import types\n",
    "import google.genai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Any, Dict, List, Union\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import asyncio\n",
    "import pprint\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import tarfile\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b6a1a",
   "metadata": {},
   "source": [
    "### === Definições Iniciais ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec89c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes dos agentes\n",
    "APP_NAME = \"full_data_pipeline_app\"\n",
    "USER_ID = \"dev_user_01\"\n",
    "SESSION_ID = \"session_01\"\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"    # [ 'gemini-2.0-flash' | 'gemini-2.0-flash-lite' | 'gemini-2.5-flash' | 'gemini-2.5-flash-lite' ]\n",
    "\n",
    "# Outras constantes\n",
    "URL = \"https://download.inep.gov.br/dados_abertos/microdados_censo_escolar_2024.zip\"    # URL para download dos dados\n",
    "DATA_DIR = \"DATA\"                                                                       # diretório local onde os dados brutos serão baixados e extraídos\n",
    "TIME = 1                                                                                # pausa em segundos entre cada ação para evitar erros de \"quota exceeded\" da API (limite de requisições por minuto)\n",
    "MAX_ITERATIONS = 3                                                                      # máximo de loops da pipeline agêntica\n",
    "\n",
    "# Chaves de Estados da sessão\n",
    "DATA_WORKSPACE = {}\n",
    "STATE_ENGINEERING_SUMMARY = \"engineering_summary\"\n",
    "STATE_PERFORMANCE_METRICS = \"performance_metrics\"\n",
    "STATE_HYPERPARAMETERS = \"hyperparameters\"\n",
    "STATE_CRITIQUE = \"critique_output\"\n",
    "REENGINEER_SIGNAL = \"REVISE DATA ENGINEERING\"\n",
    "TUNE_HYPERPARAMETERS_SIGNAL = \"REVISE HYPERPARAMETER TUNING\"\n",
    "\n",
    "# Query do usuário\n",
    "INITIAL_QUERY = (\n",
    "    f\"Verifique os dados contidos na pasta '{DATA_DIR}' e encontre o arquivo principal referente às escolas. \"\n",
    "    \"Utilize os dicionários de dados dos datasets (se existirem). \"\n",
    "    \"O objetivo é prever se uma escola possui internet. \"\n",
    "#    \"O objetivo é prever se uma escola possui água potável. \"\n",
    "    \"Selecione colunas relevantes (como a localização e infraestrutura da escola) para construir o modelo.\"\n",
    "    \"Para o workflow, utilize somente as ferramentas previamente dadas.\"\n",
    ")\n",
    "\n",
    "# Carrega variáveis de ambiente\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_GENAI_USE_VERTEXAI = os.getenv(\"GOOGLE_GENAI_USE_VERTEXAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad58f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Modelos disponíveis que suportam 'generateContent':\")\n",
    "# print(\"-------------------------------------------------\")\n",
    "\n",
    "# client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# for model in client.models.list():\n",
    "#     print(f\"Nome da API: {model.name}\")\n",
    "#     print(f\"  Nome de Exibição: {model.display_name}\")\n",
    "#     print(f\"  Descrição: {model.description}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ccef6",
   "metadata": {},
   "source": [
    "### Dowload e Extração dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6694bb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o processo de download e extração...\n",
      "--- Garantindo que o diretório 'DATA' exista. ---\n",
      "--- O arquivo 'DATA\\microdados_censo_escolar_2024.zip' já existe. Pulando o download. ---\n",
      "--- O conteúdo já parece ter sido extraído em 'DATA'. Pulando a extração. ---\n",
      "\n",
      "Processo finalizado.\n",
      "Verifique a pasta 'DATA' para ver os resultados.\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract(url: str, data_dir: str):\n",
    "    \"\"\"\n",
    "    Baixa um arquivo de uma URL, salva-o em um diretório específico e o extrai.\n",
    "\n",
    "    A função verifica se o arquivo já foi baixado e se o conteúdo já foi\n",
    "    extraído antes de executar as operações, evitando trabalho redundante.\n",
    "\n",
    "    Args:\n",
    "        url: A URL do arquivo a ser baixado.\n",
    "        data_dir: O diretório para salvar o arquivo e extrair seu conteúdo.\n",
    "    \"\"\"\n",
    "    # Garante que o diretório de destino exista.\n",
    "    print(f\"--- Garantindo que o diretório '{data_dir}' exista. ---\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Define o caminho de salvamento do arquivo dentro do diretório de dados.\n",
    "    filename = os.path.basename(url)\n",
    "    archive_path = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Verifica se o arquivo já existe para evitar um novo download.\n",
    "    if not os.path.exists(archive_path):\n",
    "        print(f\"--- Baixando arquivo de {url} para {archive_path} ---\")\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            # Lança uma exceção para respostas com erro (ex: 404, 500).\n",
    "            response.raise_for_status()\n",
    "            with open(archive_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(\"--- Download concluído com sucesso. ---\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro ao baixar o arquivo: {e}\")\n",
    "            return # Interrompe a execução se o download falhar.\n",
    "    else:\n",
    "        print(f\"--- O arquivo '{archive_path}' já existe. Pulando o download. ---\")\n",
    "\n",
    "    # Verifica se o conteúdo já foi extraído.\n",
    "    # Esta verificação assume que o arquivo .zip contém uma pasta principal\n",
    "    # com o mesmo nome do arquivo (ex: 'ml-latest-small.zip' -> 'ml-latest-small/').\n",
    "    extracted_folder_name = os.path.splitext(filename)[0]\n",
    "    expected_extracted_path = os.path.join(data_dir, extracted_folder_name)\n",
    "\n",
    "    if os.path.exists(expected_extracted_path):\n",
    "        print(f\"--- O conteúdo já parece ter sido extraído em '{data_dir}'. Pulando a extração. ---\")\n",
    "    else:\n",
    "        # Extrai o arquivo baixado para o mesmo diretório.\n",
    "        print(f\"--- Extraindo {archive_path} para {data_dir} ---\")\n",
    "        try:\n",
    "            if archive_path.endswith('.zip'):\n",
    "                with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(data_dir)\n",
    "            elif archive_path.endswith(('.tar', '.tar.gz', '.tgz', '.tar.bz2')):\n",
    "                with tarfile.open(archive_path, 'r:*') as tar_ref:\n",
    "                    tar_ref.extractall(path=data_dir)\n",
    "            else:\n",
    "                print(f\"Formato de arquivo não suportado para extração: {archive_path}\")\n",
    "                return\n",
    "\n",
    "            print(f\"--- Arquivo extraído com sucesso para '{data_dir}'. ---\")\n",
    "\n",
    "        except (zipfile.BadZipFile, tarfile.ReadError) as e:\n",
    "            print(f\"Erro ao extrair o arquivo: {e}\")\n",
    "\n",
    "print(\"Iniciando o processo de download e extração...\")\n",
    "download_and_extract(url=URL, data_dir=DATA_DIR)\n",
    "print(\"\\nProcesso finalizado.\")\n",
    "print(f\"Verifique a pasta '{DATA_DIR}' para ver os resultados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730ff56",
   "metadata": {},
   "source": [
    "### Ferramentas (Tools) dos AI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02de112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_project_files(start_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Lists all folders, subfolders, and their files within a directory.\n",
    "\n",
    "    Args:\n",
    "        start_path: The directory to start listing from.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status and a string representing the file tree.\n",
    "    \"\"\"\n",
    "    if '..' in start_path:\n",
    "        return {\"status\": \"error\", \"message\": \"Path cannot contain '..'. Access is restricted.\"}\n",
    "    try:\n",
    "        tree_string = \"\"\n",
    "        for root, dirs, files in os.walk(start_path):\n",
    "            if any(d in root for d in ['__pycache__', '.venv', 'env', '.git']):\n",
    "                continue\n",
    "            level = root.replace(start_path, '').count(os.sep)\n",
    "            indent = \" \" * 4 * level\n",
    "            tree_string += f\"{indent}{os.path.basename(root)}/\\n\"\n",
    "            sub_indent = \" \" * 4 * (level + 1)\n",
    "            for f in files:\n",
    "                tree_string += f\"{sub_indent}{f}\\n\"\n",
    "        print(f\"--- Tool: Listing files in {start_path} ---\")\n",
    "        return {\"status\": \"success\", \"file_tree\": tree_string or \"No files or directories found.\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def read_file(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads the content of a file, handling both text and PDF formats.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status and the content of the file.\n",
    "    \"\"\"\n",
    "    max_chars = 1000 # the maximum number of characters to return from the file content.\n",
    "\n",
    "    file_extension = file_path.split('.')[-1].lower()\n",
    "    try:\n",
    "        if file_extension == 'pdf':\n",
    "            with open(file_path, 'rb') as f:\n",
    "                reader = PyPDF2.PdfReader(f)\n",
    "                content = \"\"\n",
    "                for page in reader.pages:\n",
    "                    content += page.extract_text()\n",
    "        else:\n",
    "            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                content = f.read()\n",
    "\n",
    "        if len(content) > max_chars:\n",
    "            content = content[:max_chars] + \"...\"  # Truncate content\n",
    "\n",
    "        print(f\"--- Tool: Reading file {file_path} ---\")\n",
    "        return {\"status\": \"success\", \"file_path\": file_path, \"content\": content}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Error reading file {file_path}: {e}\"}\n",
    "\n",
    "\n",
    "def inspect_file_structure(file_path: str, num_rows: int, header_row: int) -> dict:\n",
    "    \"\"\"\n",
    "    Previews the top of a file to identify its structure (header, columns, delimiter).\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the dataset file (CSV or Excel).\n",
    "        num_rows: The number of rows to preview. If None, defaults to 10.\n",
    "        header_row: The 0-indexed row presumed to be the header. If None, defaults to 0.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Internal Defaults ---\n",
    "        if num_rows is None:\n",
    "            num_rows = 10\n",
    "        if header_row is None:\n",
    "            header_row = 0\n",
    "\n",
    "        extension = os.path.splitext(file_path)[-1].lower()\n",
    "        preview_args = {'header': None, 'nrows': num_rows}\n",
    "        column_args = {'header': header_row}\n",
    "\n",
    "        if extension == '.csv':\n",
    "            df_preview = pd.read_csv(file_path, **preview_args, sep=None, engine='python', encoding='latin1')\n",
    "            df_cols = pd.read_csv(file_path, **column_args, nrows=0, encoding='latin1')\n",
    "        elif extension in ['.xls', '.xlsx']:\n",
    "            df_preview = pd.read_excel(file_path, **preview_args)\n",
    "            df_cols = pd.read_excel(file_path, **column_args)\n",
    "        else:\n",
    "            return {\"status\": \"error\", \"message\": f\"Unsupported file type: '{extension}'.\"}\n",
    "\n",
    "        print(f\"--- Tool: Inspecting file structure for {file_path} ---\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"file_path\": file_path,\n",
    "            \"header_preview\": df_preview.to_string(),\n",
    "            \"column_names\": df_cols.columns.tolist()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Error inspecting file {file_path}: {e}\"}\n",
    "\n",
    "\n",
    "def query_data_dictionary(file_path: str, filter_values: List[str]) -> Dict[str, Union[str, int, List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Queries a data dictionary file to retrieve specific rows based on a filter.\n",
    "\n",
    "    This function reads a CSV or Excel file and scans every cell. It returns the\n",
    "    complete rows where at least one cell contains any of the strings in `filter_values`.\n",
    "    The search is case-insensitive. All data in the returned rows is truncated to a \n",
    "    maximum of 50 characters per cell.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the data file (CSV or Excel).\n",
    "        filter_values: A list of string values to search for anywhere in the file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the operation's status. On success, it includes the\n",
    "        retrieved data as a list of dictionaries, where each dictionary represents a\n",
    "        matching and truncated row.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return {\"status\": \"error\", \"message\": f\"File not found: {file_path}\"}\n",
    "\n",
    "        # Determine file type and read it into a DataFrame\n",
    "        extension = os.path.splitext(file_path)[-1].lower()\n",
    "        if extension == '.csv':\n",
    "            # Using 'latin1' encoding for broader compatibility with datasets\n",
    "            df = pd.read_csv(file_path, encoding='latin1')\n",
    "        elif extension in ['.xls', '.xlsx']:\n",
    "            df = pd.read_excel(file_path)\n",
    "        else:\n",
    "            return {\"status\": \"error\", \"message\": f\"Unsupported file type: '{extension}'.\"}\n",
    "\n",
    "        # Create a single regex pattern to search for any of the filter_values, case-insensitively\n",
    "        # re.escape handles special characters in the filter values\n",
    "        pattern = r'|'.join(re.escape(val) for val in filter_values)\n",
    "        \n",
    "        # Search all columns for the pattern.\n",
    "        # First, convert all data to string to apply string operations uniformly.\n",
    "        # This returns a boolean DataFrame of the same shape as `df`.\n",
    "        mask = df.astype(str).apply(lambda x: x.str.contains(pattern, case=False, na=False))\n",
    "        \n",
    "        # Keep rows where at least one column (cell) matched the pattern\n",
    "        filtered_df = df[mask.any(axis=1)]\n",
    "        # Replace all NaN values with an empty string\n",
    "        cleaned_df = filtered_df.fillna('')\n",
    "        # Truncate all values in the filtered DataFrame to 50 characters\n",
    "        truncated_df = cleaned_df.applymap(lambda x: str(x)[:50] if pd.notna(x) else x)\n",
    "        # Convert the final DataFrame to a list of dictionaries\n",
    "        records = truncated_df.to_dict(orient='records')\n",
    "        \n",
    "        print(f\"--- Tool: Searched {file_path} and found {len(records)} matching rows. ---\")\n",
    "        return {\"status\": \"success\", \"file_path\": file_path, \"data\": records}\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"An error occurred while searching {file_path}: {e}\"}\n",
    "\n",
    "\n",
    "def load_dataset(file_name: str, header_row: int, use_columns: List[str], delimiter: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads data from a CSV or Excel file into a DataFrame in the workspace.\n",
    "\n",
    "    Args:\n",
    "        file_name: The path of the file to load.\n",
    "        header_row: The 0-indexed row containing column names. If None, defaults to 0.\n",
    "        use_columns: A list of column names to load. If None, all columns are loaded.\n",
    "        delimiter: The character for separating values in a CSV. If None, defaults to ','.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Internal Defaults ---\n",
    "        if header_row is None:\n",
    "            header_row = 0\n",
    "        if delimiter is None:\n",
    "            delimiter = ','\n",
    "\n",
    "        extension = os.path.splitext(file_name)[-1].lower()\n",
    "        if extension == '.csv':\n",
    "            df = pd.read_csv(file_name, header=header_row, usecols=use_columns, sep=delimiter,\n",
    "                             low_memory=False, encoding='latin1')\n",
    "        elif extension in ['.xls', '.xlsx']:\n",
    "            df = pd.read_excel(file_name, header=header_row, usecols=use_columns)\n",
    "        else:\n",
    "            return {\"status\": \"error\", \"message\": \"Invalid file type. Must be 'csv' or 'xlsx'.\"}\n",
    "\n",
    "        df_key = f\"df_{os.path.basename(file_name).split('.')[0]}\"\n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        print(f\"--- Tool: load_dataset successful. Stored under key: {df_key} ---\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"rows_loaded\": len(df),\n",
    "            \"columns_loaded\": len(df.columns),\n",
    "            \"columns\": df.columns.tolist()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    " \n",
    "\n",
    "def preview_dataset(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Previews the first n rows of a DataFrame from the workspace.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame in the workspace.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and a string representation of the DataFrame's head.\n",
    "    \"\"\"\n",
    "    n = 10 # Number of rows to be seen\n",
    "\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    df = DATA_WORKSPACE[df_key]\n",
    "    return {\"status\": \"success\", \"df_key\": df_key, \"preview\": df.head(n).to_string()}\n",
    "\n",
    "\n",
    "def dataset_info(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Provides technical information about a DataFrame (columns, types, non-null counts).\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame in the workspace.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the DataFrame's info as a string.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    df = DATA_WORKSPACE[df_key]\n",
    "    buffer = io.StringIO()\n",
    "    df.info(buf=buffer)\n",
    "    return {\"status\": \"success\", \"df_key\": df_key, \"info\": buffer.getvalue()}\n",
    "\n",
    "\n",
    "def describe_dataset(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Provides descriptive statistics for a DataFrame in the workspace.\n",
    "    Includes statistics for both numeric and object/categorical columns.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame in the workspace.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status and a string representation of the described DataFrame.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    try:\n",
    "        df = DATA_WORKSPACE[df_key]\n",
    "        # include='all' ensures that statistics for all column types are generated.\n",
    "        description = df.describe(include='all').to_string()\n",
    "        print(f\"--- Tool: Describing dataset {df_key} ---\")\n",
    "        return {\"status\": \"success\", \"df_key\": df_key, \"description\": description}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Error describing DataFrame {df_key}: {e}\"}\n",
    "\n",
    "\n",
    "def clean_dataset(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Cleans a DataFrame by filling NaN values based on column type and removing duplicates.\n",
    "\n",
    "    - For numeric columns with more than 2 unique values, NaNs are filled with the mean.\n",
    "    - For object/categorical columns or numeric columns with 2 or fewer unique values\n",
    "      (i.e., boolean-like), NaNs are filled with the mode.\n",
    "    - Duplicate rows are removed after filling NaNs.\n",
    "    \n",
    "    The operation modifies the DataFrame in place.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame in DATA_WORKSPACE to clean.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and statistics about the cleaning process.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "\n",
    "    try:\n",
    "        df = DATA_WORKSPACE[df_key]\n",
    "        rows_before = len(df)\n",
    "        nan_count_before = int(df.isnull().sum().sum())\n",
    "        imputation_details = {}\n",
    "\n",
    "        # Iterate through each column to fill NaN values\n",
    "        for col in df.columns:\n",
    "            if df[col].isnull().any():\n",
    "                # Heuristic: Treat as numerical if dtype is numeric and it's not boolean-like\n",
    "                if pd.api.types.is_numeric_dtype(df[col]) and df[col].nunique() > 2:\n",
    "                    fill_value = df[col].mean()\n",
    "                    df[col].fillna(fill_value, inplace=True)\n",
    "                    imputation_details[col] = {\"method\": \"mean\", \"value\": fill_value}\n",
    "                # Treat as categorical/boolean for object types or low-cardinality numerics\n",
    "                else:\n",
    "                    fill_value = df[col].mode()[0]\n",
    "                    df[col].fillna(fill_value, inplace=True)\n",
    "                    imputation_details[col] = {\"method\": \"mode\", \"value\": fill_value}\n",
    "\n",
    "        # Handle duplicates after NaN imputation\n",
    "        rows_after_fill = len(df)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        rows_after_dedup = len(df)\n",
    "        \n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"rows_before\": rows_before,\n",
    "            \"rows_after\": rows_after_dedup,\n",
    "            \"nan_values_filled\": nan_count_before,\n",
    "            \"duplicates_removed\": rows_after_fill - rows_after_dedup,\n",
    "            \"imputation_details\": imputation_details\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def convert_to_categorical(df_key: str, columns_to_convert: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Converts specified columns in a DataFrame to the 'category' dtype. Modifies in place.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame to modify.\n",
    "        columns_to_convert: A list of column names to convert.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary confirming the status and listing the converted columns.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    try:\n",
    "        df = DATA_WORKSPACE[df_key]\n",
    "        converted = []\n",
    "        not_found = []\n",
    "        for col in columns_to_convert:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype('category')\n",
    "                converted.append(col)\n",
    "            else:\n",
    "                not_found.append(col)\n",
    "        \n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"converted_columns\": converted,\n",
    "            \"columns_not_found\": not_found\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "\n",
    "def convert_to_int(df_key: str, columns_to_convert: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Converts specified columns in a DataFrame to the integer dtype. Modifies in place.\n",
    "\n",
    "    Note: This will fail if a column contains non-numeric values or NaNs.\n",
    "    The calling code should handle such potential errors.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame to modify in DATA_WORKSPACE.\n",
    "        columns_to_convert: A list of column names to convert.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary confirming the status and listing the converted columns.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "\n",
    "    try:\n",
    "        df = DATA_WORKSPACE[df_key]\n",
    "        converted = []\n",
    "        not_found = []\n",
    "        \n",
    "        for col in columns_to_convert:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(int)\n",
    "                converted.append(col)\n",
    "            else:\n",
    "                not_found.append(col)\n",
    "        \n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"converted_columns\": converted,\n",
    "            \"columns_not_found\": not_found\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # This will catch errors like trying to convert a column with NaN or non-numeric strings\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def split_features_target(df_key: str, target_column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into features (X) and target (y). Stores them in the workspace.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame to split.\n",
    "        target_column: The name of the target column (y).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the new keys for features (X) and target (y).\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    df = DATA_WORKSPACE[df_key]\n",
    "    if target_column not in df.columns:\n",
    "        return {\"status\": \"error\", \"message\": f\"Target column '{target_column}' not in DataFrame.\"}\n",
    "    \n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    X_key = f\"X_{df_key}\"\n",
    "    y_key = f\"y_{df_key}\"\n",
    "    \n",
    "    DATA_WORKSPACE[X_key] = X\n",
    "    DATA_WORKSPACE[y_key] = y\n",
    "    \n",
    "    return {\"status\": \"success\", \"features_key\": X_key, \"target_key\": y_key}\n",
    "\n",
    "\n",
    "def train_test_split_data_for_classifier(X_key: str, y_key: str, test_size: float, random_state: int) -> dict:\n",
    "    \"\"\"\n",
    "    Splits features (X) and target (y) into training and testing sets.\n",
    "    Automatically sets the `stratify` parameter to y, so that the target is not unbalanced.\n",
    "\n",
    "    Args:\n",
    "        X_key: The workspace key for the features DataFrame (X).\n",
    "        y_key: The workspace key for the target Series (y).\n",
    "        test_size: Proportion for the test split.\n",
    "        random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the keys for X_train, X_test, y_train, and y_test.\n",
    "    \"\"\"\n",
    "    if X_key not in DATA_WORKSPACE or y_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"Feature key '{X_key}' or target key '{y_key}' not found.\"}\n",
    "        \n",
    "    X = DATA_WORKSPACE[X_key]\n",
    "    y = DATA_WORKSPACE[y_key]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=y) # stratify para que o target não seja desbalanceado\n",
    "    \n",
    "    keys = {\n",
    "        \"X_train\": f\"{X_key}_train\", \"X_test\": f\"{X_key}_test\",\n",
    "        \"y_train\": f\"{y_key}_train\", \"y_test\": f\"{y_key}_test\"\n",
    "    }\n",
    "    \n",
    "    DATA_WORKSPACE[keys[\"X_train\"]] = X_train\n",
    "    DATA_WORKSPACE[keys[\"X_test\"]] = X_test\n",
    "    DATA_WORKSPACE[keys[\"y_train\"]] = y_train\n",
    "    DATA_WORKSPACE[keys[\"y_test\"]] = y_test\n",
    "    \n",
    "    return {\"status\": \"success\", \"data_keys\": keys}\n",
    "\n",
    "\n",
    "def train_test_split_data_for_regressor(X_key: str, y_key: str, test_size: float, random_state: int) -> dict:\n",
    "    \"\"\"\n",
    "    Splits features (X) and target (y) into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        X_key: The workspace key for the features DataFrame (X).\n",
    "        y_key: The workspace key for the target Series (y).\n",
    "        test_size: Proportion for the test split.\n",
    "        random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the keys for X_train, X_test, y_train, and y_test.\n",
    "    \"\"\"\n",
    "    if X_key not in DATA_WORKSPACE or y_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"Feature key '{X_key}' or target key '{y_key}' not found.\"}\n",
    "        \n",
    "    X = DATA_WORKSPACE[X_key]\n",
    "    y = DATA_WORKSPACE[y_key]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=random_state)\n",
    "    \n",
    "    keys = {\n",
    "        \"X_train\": f\"{X_key}_train\", \"X_test\": f\"{X_key}_test\",\n",
    "        \"y_train\": f\"{y_key}_train\", \"y_test\": f\"{y_key}_test\"\n",
    "    }\n",
    "    \n",
    "    DATA_WORKSPACE[keys[\"X_train\"]] = X_train\n",
    "    DATA_WORKSPACE[keys[\"X_test\"]] = X_test\n",
    "    DATA_WORKSPACE[keys[\"y_train\"]] = y_train\n",
    "    DATA_WORKSPACE[keys[\"y_test\"]] = y_test\n",
    "    \n",
    "    return {\"status\": \"success\", \"data_keys\": keys}\n",
    "    \n",
    "\n",
    "def train_xgboost_model(X_train_key: str, y_train_key: str, model_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Trains an XGBoost model (Classifier or Regressor) with a fixed set of\n",
    "    hyperparameters and stores it in the workspace.\n",
    "\n",
    "    Args:\n",
    "        X_train_key: The workspace key for the training features (X_train).\n",
    "        y_train_key: The workspace key for the training target (y_train).\n",
    "        model_type: The type of model to train, either 'classifier' or 'regressor'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the status, the key for the trained model, and the\n",
    "        hyperparameters that were used.\n",
    "    \"\"\"\n",
    "    # Check if the specified training data exists in the workspace\n",
    "    if X_train_key not in DATA_WORKSPACE or y_train_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Training data keys not found in workspace.\"}\n",
    "\n",
    "    try:\n",
    "        X_train = DATA_WORKSPACE[X_train_key]\n",
    "        y_train = DATA_WORKSPACE[y_train_key]\n",
    "\n",
    "        # Define the fixed set of hyperparameters\n",
    "        params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'enable_categorical': True,\n",
    "            'random_state': 42 # Added for reproducibility\n",
    "        }\n",
    "\n",
    "        # Select and instantiate the model based on the model_type parameter\n",
    "        if model_type.lower() == 'classifier':\n",
    "            model = XGBClassifier(**params)\n",
    "            model_key = \"xgb_classifier_model\"\n",
    "        elif model_type.lower() == 'regressor':\n",
    "            model = XGBRegressor(**params)\n",
    "            model_key = \"xgb_regressor_model\"\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": \"Invalid model_type. Please choose 'classifier' or 'regressor'.\"\n",
    "            }\n",
    "\n",
    "        # Train the selected model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Store the trained model in the workspace\n",
    "        DATA_WORKSPACE[model_key] = model\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"model_key\": model_key,\n",
    "            \"hyperparameters_used\": params\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def evaluate_classifier_performance(model_key: str, X_test_key: str, y_test_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a classifier model using precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        model_key: The workspace key of the trained classifier model.\n",
    "        X_test_key: The workspace key of the test features (X_test).\n",
    "        y_test_key: The workspace key of the true test target values (y_test).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the status and performance metrics.\n",
    "    \"\"\"\n",
    "    if model_key not in DATA_WORKSPACE or X_test_key not in DATA_WORKSPACE or y_test_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Model or test data keys not found in workspace.\"}\n",
    "        \n",
    "    try:\n",
    "        model = DATA_WORKSPACE[model_key]\n",
    "        X_test = DATA_WORKSPACE[X_test_key]\n",
    "        y_test = DATA_WORKSPACE[y_test_key]\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        precision = precision_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"metrics\": {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to evaluate classifier: {e}\"}\n",
    "\n",
    "\n",
    "def evaluate_regressor_performance(model_key: str, X_test_key: str, y_test_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a regressor model using MAE, RMSE, and R-squared.\n",
    "\n",
    "    Args:\n",
    "        model_key: The workspace key of the trained regressor model.\n",
    "        X_test_key: The workspace key of the test features (X_test).\n",
    "        y_test_key: The workspace key of the true test target values (y_test).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the status and performance metrics.\n",
    "    \"\"\"\n",
    "    if model_key not in DATA_WORKSPACE or X_test_key not in DATA_WORKSPACE or y_test_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Model or test data keys not found in workspace.\"}\n",
    "\n",
    "    try:\n",
    "        model = DATA_WORKSPACE[model_key]\n",
    "        X_test = DATA_WORKSPACE[X_test_key]\n",
    "        y_test = DATA_WORKSPACE[y_test_key]\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"metrics\": {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to evaluate regressor: {e}\"}\n",
    "\n",
    "\n",
    "def hyperparameter_search_xgboost(X_train_key: str, y_train_key: str, model_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Performs a hyperparameter grid search for an XGBoost model using cross-validation.\n",
    "\n",
    "    Args:\n",
    "        X_train_key: The workspace key for the training features (X_train).\n",
    "        y_train_key: The workspace key for the training target (y_train).\n",
    "        model_type: The type of model to tune, either 'classifier' or 'regressor'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status, the key for the best model found, and the best parameters.\n",
    "    \"\"\"\n",
    "    if X_train_key not in DATA_WORKSPACE or y_train_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Training data keys not found in workspace.\"}\n",
    "    if model_type not in ['classifier', 'regressor']:\n",
    "        return {\"status\": \"error\", \"message\": \"model_type must be 'classifier' or 'regressor'.\"}\n",
    "\n",
    "    try:\n",
    "        X_train = DATA_WORKSPACE[X_train_key]\n",
    "        y_train = DATA_WORKSPACE[y_train_key]\n",
    "\n",
    "        # Convert data to DMatrix for XGBoost efficiency\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "\n",
    "        # Define hyperparameter grid\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'eta': np.logspace(start=0.01, stop=0.3, num=5),\n",
    "            'subsample': [0.6, 0.8],\n",
    "            'colsample_bytree': [0.6, 0.8]\n",
    "        }\n",
    "        \n",
    "        # Configure search based on model type\n",
    "        if model_type == 'regressor':\n",
    "            objective = 'reg:squarederror'\n",
    "            eval_metric = 'rmse'\n",
    "        else: # classifier\n",
    "            if y_train.nunique() == 2:\n",
    "                objective = 'binary:logistic'\n",
    "                eval_metric = 'logloss'\n",
    "            else:\n",
    "                objective = 'multi:softmax'\n",
    "                eval_metric = 'mlogloss'\n",
    "\n",
    "        # Placeholders for results\n",
    "        best_score = float(\"inf\")\n",
    "        best_params_combo = {}\n",
    "        \n",
    "        # Create all combinations for the grid search\n",
    "        search_space = list(product(*param_grid.values()))\n",
    "\n",
    "        print(f\"--- Tool: Starting XGBoost hyperparameter search for {model_type} ({len(search_space)} combinations) ---\")\n",
    "\n",
    "        for params_tuple in search_space:\n",
    "            params = dict(zip(param_grid.keys(), params_tuple))\n",
    "            params['objective'] = objective\n",
    "            \n",
    "            if model_type == 'classifier' and y_train.nunique() > 2:\n",
    "                 params['num_class'] = y_train.nunique()\n",
    "\n",
    "            # Execute cross-validation\n",
    "            cv_results = xgb.cv(\n",
    "                params=params,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=500,\n",
    "                nfold=3,\n",
    "                metrics=eval_metric,\n",
    "                early_stopping_rounds=25,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            \n",
    "            # Get the best score from this CV run (lowest error)\n",
    "            current_score = cv_results[f'test-{eval_metric}-mean'].min()\n",
    "            \n",
    "            # Update best score and params if current run is better\n",
    "            if current_score < best_score:\n",
    "                best_score = current_score\n",
    "                best_params_combo = params\n",
    "                # Find the optimal number of boosting rounds\n",
    "                best_iteration = cv_results[f'test-{eval_metric}-mean'].idxmin()\n",
    "                best_params_combo['n_estimators'] = best_iteration\n",
    "\n",
    "        # Train the final model with the absolute best parameters\n",
    "        final_model_params = {k: v for k, v in best_params_combo.items() if k not in ['objective', 'num_class']}\n",
    "        \n",
    "        if model_type == 'regressor':\n",
    "            model = XGBRegressor(**final_model_params, objective=objective, enable_categorical=True)\n",
    "        else:\n",
    "            model = XGBClassifier(**final_model_params, objective=objective, enable_categorical=True)\n",
    "            \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        model_key = f\"xgb_{model_type}_tuned_model\"\n",
    "        DATA_WORKSPACE[model_key] = model\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\", \n",
    "            \"model_key\": model_key,\n",
    "            \"best_params_found\": best_params_combo\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Hyperparameter search failed: {e}\"}\n",
    "\n",
    "\n",
    "def save_model_and_metadata(model_key: str, X_train_key: str, hyperparameters: Dict[str, Any], model_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Saves the trained model and its metadata (columns, hyperparameters).\n",
    "    The output folder is set internally to 'trained_model_artifacts'.\n",
    "\n",
    "    Args:\n",
    "        model_key: The workspace key for the trained model.\n",
    "        X_train_key: The workspace key for the training features (X_train).\n",
    "        hyperparameters: A dictionary of hyperparameters used for training.\n",
    "        model_type: The type of model ('classifier' or 'regressor').\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status and paths to the saved files.\n",
    "    \"\"\"\n",
    "    # --- Internal Defaults ---\n",
    "    output_folder = \"trained_model_artifacts\"\n",
    "\n",
    "    if model_key not in DATA_WORKSPACE or X_train_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Model or training data key not found in workspace.\"}\n",
    "\n",
    "    try:\n",
    "        # Create the output directory if it doesn't already exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # 1. Save the XGBoost model using its native method\n",
    "        model = DATA_WORKSPACE[model_key]\n",
    "        model_path = os.path.join(output_folder, f\"{model_key}.json\")\n",
    "        model.save_model(model_path)\n",
    "\n",
    "        # 2. Prepare and save the metadata\n",
    "        X_train = DATA_WORKSPACE[X_train_key]\n",
    "        # Ensure all hyperparameter values are JSON serializable\n",
    "        serializable_hyperparameters = {k: (v.item() if hasattr(v, 'item') else v) for k, v in hyperparameters.items()}\n",
    "        \n",
    "        metadata = {\n",
    "            \"model_type\": model_type,\n",
    "            \"feature_columns\": X_train.columns.tolist(),\n",
    "            \"hyperparameters\": serializable_hyperparameters,\n",
    "        }\n",
    "        metadata_path = os.path.join(output_folder, \"model_metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "\n",
    "        print(f\"--- Tool: Saved model to {model_path} and metadata to {metadata_path} ---\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"model_path\": model_path,\n",
    "            \"metadata_path\": metadata_path\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def exit_loop(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"\n",
    "    Signals the main agent loop to stop iterating.\n",
    "\n",
    "    Args:\n",
    "        tool_context: The context object provided by the ADK framework.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary confirming that the exit signal has been sent.\n",
    "    \"\"\"\n",
    "    print(f\"--- [Tool Call] exit_loop activated by {tool_context.agent_name} ---\")\n",
    "    tool_context.actions.escalate = True\n",
    "    return {\"status\": \"success\", \"message\": \"Exit signal sent to the main loop.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda8d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista das ferramentas disponíveis para o agente de Data Engineering\n",
    "ENGINEERING_TOOLS = [list_project_files, inspect_file_structure, query_data_dictionary, read_file]\n",
    "\n",
    "# Lista das ferramentas disponíveis para o agente de Data Science\n",
    "SCIENCE_TOOLS = [load_dataset, dataset_info, describe_dataset, preview_dataset, clean_dataset, convert_to_categorical, convert_to_int, \n",
    "                 split_features_target, train_test_split_data_for_classifier, train_test_split_data_for_regressor, train_xgboost_model,\n",
    "                 evaluate_classifier_performance, evaluate_regressor_performance, hyperparameter_search_xgboost, save_model_and_metadata]\n",
    "\n",
    "# Lista das ferramentas disponíveis para o agente de Avaliação\n",
    "CRITIQUE_TOOLS = [exit_loop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb1a05",
   "metadata": {},
   "source": [
    "### Definição dos Agentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b818ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. O Agente \"Engenheiro de Dados\"\n",
    "data_engineer_agent = LlmAgent(\n",
    "    name=\"DataEngineerAgent\",\n",
    "    model=GEMINI_MODEL,\n",
    "    instruction=f\"\"\"\n",
    "    You are a highly efficient Data Engineer AI. Your goal is to logically identify and prepare the features for a machine learning model. You must follow these steps in order:\n",
    "\n",
    "    **1. Explore and Inspect:**\n",
    "        - Use `list_project_files` to identify the primary dataset and the data dictionary file.\n",
    "        - Use `inspect_file_structure` on the **primary dataset** to get the exact list of all available column names and determine the `header_row` and `delimiter`.\n",
    "\n",
    "    **2. Make a Preliminary Selection:**\n",
    "        - From the full list of column names you just obtained, make a **preliminary selection** of approximately 10 columns that seem most relevant for the ML model based on their names alone.\n",
    "\n",
    "    **3. Validate Selection with Data Dictionary:**\n",
    "        - Use the `query_data_dictionary` tool on the **data dictionary file** to retrieve the descriptions **only for the ~10 columns chosen in the preliminary selection**.\n",
    "          This allows you to confirm they are suitable.\n",
    "        - If needed, use the `read_file` function to read relevant information about the dataset from text or PDF files.\n",
    "\n",
    "    **4. Finalize Features and Prepare Output:**\n",
    "        - Review the descriptions returned by `query_data_dictionary`. Based on their meanings, confirm or revise your list to create the final set of features.\n",
    "        - The session state key '{STATE_CRITIQUE}' may contain feedback. If it contains the signal \"{REENGINEER_SIGNAL}\", you **MUST** choose a **different combination of features**.\n",
    "        - Your final output for this turn **MUST** be a single, valid JSON object that the next agent will use to call the `load_dataset` function.\n",
    "        - It must contain the keys `file_name`, `header_row`, `use_columns`, and optionally `delimiter`.\n",
    "        - Example: {{\"file_name\": \"path/to/data.csv\", \"header_row\": 0, \"use_columns\": [\"col1\", \"col2\", \"col3\"], \"delimiter\": \";\"}}\n",
    "    \"\"\",\n",
    "    tools=ENGINEERING_TOOLS,\n",
    "    output_key=STATE_ENGINEERING_SUMMARY\n",
    ")\n",
    "\n",
    "# 2. O Agente \"Cientista de Dados\"\n",
    "data_scientist_agent = LlmAgent(\n",
    "    name=\"DataScientistAgent\",\n",
    "    model=GEMINI_MODEL,\n",
    "    instruction=f\"\"\"\n",
    "    You are a methodical Data Scientist AI. Your task is to preprocess data, train a model, evaluate it, and save the final artifacts. You must follow these steps in order:\n",
    "\n",
    "    **1. Load Data & Initial Analysis:**\n",
    "        - Get the data loading parameters from the session state key '{STATE_ENGINEERING_SUMMARY}'.\n",
    "        - Call the `load_dataset` tool. This returns a dictionary containing the `df_key`, which you **MUST** use to reference the dataset in all subsequent steps.\n",
    "        - **Crucial Analysis & Verification Step:** After loading, use `dataset_info`, `preview_dataset`, and `describe_dataset` on the `df_key` to understand the data's structure,\n",
    "          content, and distribution before proceeding.\n",
    "        - **Evaluate columns for a high proportion of missing values (NaNs). Columns with excessive NaNs are candidates for removal, as listwise deletion could discard a substantial\n",
    "          portion of the dataset, while imputation might introduce significant bias.**\n",
    "        \n",
    "    **2. Preprocess and Split:**\n",
    "        - Use the `clean_dataset` tool on the `df_key`.\n",
    "        - Use `convert_to_categorical` on the `df_key` for columns that are not numerical.\n",
    "        - Use `convert_to_int` on the `df_key` for columns that are supposed to be integers and NOT floats (e.g.: 0.0 and 1.0).\n",
    "        - Use `split_features_target` to separate features (X) and target (y).\n",
    "        - Based on the target variable, use either `train_test_split_data_for_classifier` or `train_test_split_data_for_regressor`.\n",
    "\n",
    "    **3. Train Model:**\n",
    "        - **Decision Point:** Check the session state key '{STATE_CRITIQUE}' for a decision from the previous loop.\n",
    "        - **If the decision was \"{TUNE_HYPERPARAMETERS_SIGNAL}\":** You **MUST** call the `hyperparameter_search_xgboost` tool. Determine the `model_type` ('classifier' or 'regressor')\n",
    "          based on the split function you used.\n",
    "        - **Otherwise (first run):** You **MUST** call the `train_xgboost_model` tool. Determine the `model_type` ('classifier' or 'regressor') based on which split function you used\n",
    "          in the previous step. This tool uses a fixed set of default hyperparameters internally.\n",
    "        - **Crucially, you MUST capture the output of this step, as it contains the `model_key` and the `hyperparameters_used` or `best_params_found` needed for the next steps.**\n",
    "\n",
    "    **4. Evaluate Model:**\n",
    "        - Use the `model_key` from the previous step to call either `evaluate_classifier_performance` or `evaluate_regressor_performance`.\n",
    "        - Capture the resulting performance metrics dictionary.\n",
    "        \n",
    "    **5. Save Artifacts:**\n",
    "        - After training and evaluation, you **MUST** save the results by calling the `save_model_and_metadata` tool.\n",
    "        - Provide the following arguments:\n",
    "            - `model_key`: The key of the model you just trained.\n",
    "            - `X_train_key`: The key for the training features (e.g., 'X_df_matriz_train').\n",
    "            - `hyperparameters`: The dictionary of hyperparameters you captured from the training step (either `hyperparameters_used` or `best_params_found`).\n",
    "            - `model_type`: The type of model you trained ('classifier' or 'regressor').\n",
    "\n",
    "    **6. Final Output:**\n",
    "        - Your final output for this turn **MUST** be the complete dictionary of performance metrics returned by the evaluation tool in Step 4.\n",
    "    \"\"\",\n",
    "    tools=SCIENCE_TOOLS,\n",
    "    output_key=STATE_PERFORMANCE_METRICS\n",
    ")\n",
    "\n",
    "# 3. O Agente \"Avaliador\"\n",
    "critique_agent = LlmAgent(\n",
    "    name=\"CritiqueAgent\",\n",
    "    model=GEMINI_MODEL,\n",
    "    instruction=f\"\"\"\n",
    "    You are a decisive Machine Learning Model Critic. Your role is to analyze model performance and determine the next action with structured output. You must follow these steps in order:\n",
    "\n",
    "    **1. Review Performance:**\n",
    "        - Analyze the performance dictionary from the session state key '{STATE_PERFORMANCE_METRICS}'.\n",
    "        - First, identify the primary metric ('F1-Score' or 'R-squared').\n",
    "        - Apply the following logic to make your decision:\n",
    "          - if score >= 0.8: Success!\n",
    "          - elif 0.8 > score >= 0.6: Moderate performance. Trigger hyperparameter re-tuning.\n",
    "          - elif score < 0.6: Poor performance. Signal feature re-engineering.\n",
    "\n",
    "    **2. Take Action:**\n",
    "        - **In the SUCCESS case:** Your ONLY action is to call the `exit_loop` tool. Do NOT output any JSON.\n",
    "        - **For re-tuning:** Your output MUST be a single JSON object: {{\"decision\": \"{TUNE_HYPERPARAMETERS_SIGNAL}\", \"reason\": \"Performance is moderate, initiating a full hyperparameter search.\"}}\n",
    "        - **For re-engineering:** Your output MUST be a single JSON object: {{\"decision\": \"{REENGINEER_SIGNAL}\", \"reason\": \"Feature selection seems inadequate.\"}}\n",
    "    \"\"\",\n",
    "    tools=CRITIQUE_TOOLS,\n",
    "    output_key=STATE_CRITIQUE\n",
    ")\n",
    "\n",
    "# 4. O Agente \"Orquestrador\"\n",
    "main_pipeline_agent = LoopAgent(\n",
    "    name=\"MainPipelineAgent\",\n",
    "    sub_agents=[\n",
    "        data_engineer_agent,\n",
    "        data_scientist_agent,\n",
    "        critique_agent\n",
    "    ],\n",
    "    max_iterations=MAX_ITERATIONS # Limite de loops da pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6717cd",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9dfe73f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_pipeline():\n",
    "    \"\"\"Configures and runs the complete agent pipeline.\"\"\"\n",
    "    session_service = InMemorySessionService()\n",
    "    session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
    "    runner = Runner(agent=main_pipeline_agent, app_name=APP_NAME, session_service=session_service)\n",
    "\n",
    "    print(f\"--- STARTING PIPELINE WITH QUERY ---\\n'{INITIAL_QUERY}'\\n\")\n",
    "    content = types.Content(role='user', parts=[types.Part(text=INITIAL_QUERY)])\n",
    "    \n",
    "    try:\n",
    "        async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
    "            # Skip empty events\n",
    "            if not event.content or not event.content.parts:\n",
    "                continue\n",
    "\n",
    "            processed_tool_part = False\n",
    "\n",
    "            for part in event.content.parts:\n",
    "                # 1. Check for a tool call (code the agent wants to run)\n",
    "                if hasattr(part, 'executable_code') and part.executable_code:\n",
    "                    print(f\"\\n>> {event.author} is calling a tool:\")\n",
    "                    print(\"```python\")\n",
    "                    print(part.executable_code.code)\n",
    "                    print(\"```\")\n",
    "                    processed_tool_part = True\n",
    "\n",
    "                # 2. Check for the result of a tool call\n",
    "                elif hasattr(part, 'code_execution_result') and part.code_execution_result:\n",
    "                    output_str = pprint.pformat(part.code_execution_result.output)\n",
    "                    print(f\"\\n>> Tool result for {event.author}:\")\n",
    "                    print(output_str)\n",
    "                    processed_tool_part = True\n",
    "\n",
    "            # 3. If it wasn't a tool part, it's likely a \"thought\" from a sub-agent.\n",
    "            # The logic for accumulating a final_response from the main agent has been removed.\n",
    "            if not processed_tool_part:\n",
    "                for part in event.content.parts:\n",
    "                    if hasattr(part, 'text') and part.text:\n",
    "                        text_content = part.text.strip()\n",
    "                        # We only print the \"thoughts\" of the sub-agents, not the main orchestrator.\n",
    "                        if text_content and event.author != main_pipeline_agent.name:\n",
    "                            print(f\"\\n>> {event.author} is thinking...\\n   {text_content}\")\n",
    "            \n",
    "            # Pause for TIME seconds after processing each event to respect rate limits.\n",
    "            await asyncio.sleep(TIME)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- AN ERROR OCCURRED ---\\n\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # The final print for 'final_response' has been removed.\n",
    "    print(\"\\n--- PIPELINE FINISHED ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf5116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING PIPELINE WITH QUERY ---\n",
      "'Verifique os dados contidos na pasta 'DATA' e encontre o arquivo principal referente às escolas. Utilize os dicionários de dados dos datasets (se existirem). O objetivo é prever se uma escola possui internet. Selecione colunas relevantes (como a localização e infraestrutura da escola) para construir o modelo.Para o workflow, utilize somente as ferramentas previamente dadas.'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   Okay, I understand the objective. I need to find the main school dataset within the 'DATA' folder, use the data dictionary if available, and select relevant columns for predicting internet access in schools. I will use the available tools to achieve this.\n",
      "\n",
      "**Step 1: Explore and Inspect**\n",
      "\n",
      "First, I'll list the files in the 'DATA' folder to identify the datasets and data dictionaries.\n",
      "--- Tool: Listing files in DATA ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   Okay, it seems like the main dataset is likely `microdados_ed_basica_2024.csv` located in `DATA/microdados_censo_escolar_2024/dados/`. The data dictionary is `dicionário_dados_educação_básica.xlsx` located in `DATA/microdados_censo_escolar_2024/Anexos/ANEXO I - Dicionário de Dados/`.\n",
      "\n",
      "Now, let's inspect the structure of the main dataset to get the column names and the header row.\n",
      "--- Tool: Inspecting file structure for DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   Okay, the column names are separated by semicolons (`;`). That's important.\n",
      "\n",
      "**Step 2: Make a Preliminary Selection**\n",
      "\n",
      "Based on the column names, here's a preliminary selection of columns that seem relevant for predicting internet access:\n",
      "\n",
      "1.  `NO_REGIAO`: Region of the school.\n",
      "2.  `NO_UF`: State of the school.\n",
      "3.  `NO_MUNICIPIO`: Municipality of the school.\n",
      "4.  `TP_DEPENDENCIA`: Type of administrative dependence (e.g., Federal, State, Municipal).\n",
      "5.  `TP_LOCALIZACAO`: Type of location (e.g., Urban, Rural).\n",
      "6.  `IN_AGUA_POTAVEL`: Indicates if the school has potable water.\n",
      "7.  `IN_ENERGIA_REDE_PUBLICA`: Indicates if the school is connected to the public power grid.\n",
      "8.  `IN_ESGOTO_REDE_PUBLICA`: Indicates if the school is connected to the public sewage system.\n",
      "9.  `IN_BIBLIOTECA`: Indicates if the school has a library.\n",
      "10. `QT_SALAS_UTILIZADAS`: Number of classrooms used.\n",
      "11. `IN_INTERNET`: This is our target variable, indicating internet access.\n",
      "\n",
      "**Step 3: Validate Selection with Data Dictionary**\n",
      "\n",
      "Now, let's use the `query_data_dictionary` tool to get the descriptions of these columns from the data dictionary file.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jean\\AppData\\Local\\Temp\\ipykernel_19860\\2522502380.py:147: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  cleaned_df = filtered_df.fillna('')\n",
      "C:\\Users\\Jean\\AppData\\Local\\Temp\\ipykernel_19860\\2522502380.py:149: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  truncated_df = cleaned_df.applymap(lambda x: str(x)[:50] if pd.notna(x) else x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: Searched DATA/microdados_censo_escolar_2024/Anexos/ANEXO I - Dicionário de Dados/dicionário_dados_educação_básica.xlsx and found 22 matching rows. ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   The data dictionary confirms the meaning of the selected columns.\n",
      "\n",
      "**Step 4: Finalize Features and Prepare Output**\n",
      "\n",
      "Based on the data dictionary results, I'll make a slight adjustment to the feature list. I'll replace `QT_SALAS_UTILIZADAS` with `QT_SALAS_UTILIZADAS_DENTRO` and `QT_SALAS_UTILIZADAS_FORA` as they provide a more detailed breakdown. I will also add `TP_LOCALIZACAO_DIFERENCIADA` as it might be important.\n",
      "\n",
      "Here's the final feature list:\n",
      "\n",
      "1.  `NO_REGIAO`\n",
      "2.  `NO_UF`\n",
      "3.  `NO_MUNICIPIO`\n",
      "4.  `TP_DEPENDENCIA`\n",
      "5.  `TP_LOCALIZACAO`\n",
      "6.  `TP_LOCALIZACAO_DIFERENCIADA`\n",
      "7.  `IN_AGUA_POTAVEL`\n",
      "8.  `IN_ENERGIA_REDE_PUBLICA`\n",
      "9.  `IN_ESGOTO_REDE_PUBLICA`\n",
      "10. `IN_BIBLIOTECA`\n",
      "11. `QT_SALAS_UTILIZADAS_DENTRO`\n",
      "12. `QT_SALAS_UTILIZADAS_FORA`\n",
      "13. `IN_INTERNET` (Target)\n",
      "\n",
      "Now, I'll prepare the JSON output for the `load_dataset` function.\n",
      "--- Tool: Reading file DATA/microdados_censo_escolar_2024/dados/md5_microdados_ed_basica_2024.txt ---\n",
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   ```json\n",
      "{\n",
      "  \"file_name\": \"DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv\",\n",
      "  \"header_row\": 0,\n",
      "  \"use_columns\": [\n",
      "    \"NO_REGIAO\",\n",
      "    \"NO_UF\",\n",
      "    \"NO_MUNICIPIO\",\n",
      "    \"TP_DEPENDENCIA\",\n",
      "    \"TP_LOCALIZACAO\",\n",
      "    \"TP_LOCALIZACAO_DIFERENCIADA\",\n",
      "    \"IN_AGUA_POTAVEL\",\n",
      "    \"IN_ENERGIA_REDE_PUBLICA\",\n",
      "    \"IN_ESGOTO_REDE_PUBLICA\",\n",
      "    \"IN_BIBLIOTECA\",\n",
      "    \"QT_SALAS_UTILIZADAS_DENTRO\",\n",
      "    \"QT_SALAS_UTILIZADAS_FORA\",\n",
      "    \"IN_INTERNET\"\n",
      "  ],\n",
      "  \"delimiter\": \";\"\n",
      "}\n",
      "```\n",
      "\n",
      ">> CritiqueAgent is thinking...\n",
      "   ```json\n",
      "{\n",
      "\"decision\": \"REVISE DATA ENGINEERING\",\n",
      "\"reason\": \"Feature selection seems inadequate.\"\n",
      "}\n",
      "```\n",
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   Okay, the critique agent has signaled \"REVISE DATA ENGINEERING\", meaning the previous feature selection was inadequate. I need to choose a different combination of features.\n",
      "\n",
      "Let's re-think the feature selection. The previous selection focused on location, infrastructure, and basic amenities. Perhaps I should include more features related to the school's resources and teaching facilities.\n",
      "\n",
      "Here's a revised selection:\n",
      "\n",
      "1.  `NO_REGIAO`: Region of the school.\n",
      "2.  `TP_DEPENDENCIA`: Type of administrative dependence.\n",
      "3.  `TP_LOCALIZACAO`: Type of location (Urban/Rural).\n",
      "4.  `IN_BIBLIOTECA`: Indicates if the school has a library.\n",
      "5.  `IN_LABORATORIO_INFORMATICA`: Indicates if the school has a computer lab.\n",
      "6.  `IN_QUADRA_ESPORTES`: Indicates if the school has a sports court.\n",
      "7.  `IN_REFEITORIO`: Indicates if the school has a cafeteria.\n",
      "8.  `QT_EQUIP_COMPUTADOR`: Number of computers in the school.\n",
      "9.  `IN_BANDA_LARGA`: Indicates if the school has broadband internet access.\n",
      "10. `QT_PROF_ADMINISTRATIVOS`: Number of administrative staff.\n",
      "11. `IN_ALIMENTACAO`: Indicates if the school offers meals.\n",
      "12. `QT_MAT_BAS`: Total number of students.\n",
      "13. `IN_INTERNET`: (Target variable)\n",
      "\n",
      "Now, let's prepare the JSON output with these revised features.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"file_name\": \"DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv\",\n",
      "  \"header_row\": 0,\n",
      "  \"use_columns\": [\n",
      "    \"NO_REGIAO\",\n",
      "    \"TP_DEPENDENCIA\",\n",
      "    \"TP_LOCALIZACAO\",\n",
      "    \"IN_BIBLIOTECA\",\n",
      "    \"IN_LABORATORIO_INFORMATICA\",\n",
      "    \"IN_QUADRA_ESPORTES\",\n",
      "    \"IN_REFEITORIO\",\n",
      "    \"QT_EQUIP_COMPUTADOR\",\n",
      "    \"IN_BANDA_LARGA\",\n",
      "    \"QT_PROF_ADMINISTRATIVOS\",\n",
      "    \"IN_ALIMENTACAO\",\n",
      "    \"QT_MAT_BAS\",\n",
      "    \"IN_INTERNET\"\n",
      "  ],\n",
      "  \"delimiter\": \";\"\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   The `load_dataset` tool returned an error: `Usecols do not match columns, columns expected but not found: ['QT_EQUIP_COMPUTADOR']`. This means the column `QT_EQUIP_COMPUTADOR` does not exist in the CSV file or was misnamed.\n",
      "\n",
      "I need to revise the feature list again and choose an existing column related to the number of computers. Looking back at the `inspect_file_structure` output, I see the column `IN_COMPUTADOR` (Indicates if the school has computers) and `QT_DESKTOP_ALUNO` (Number of desktop computers for students). I'll use `IN_COMPUTADOR` instead of `QT_EQUIP_COMPUTADOR`.\n",
      "\n",
      "Here's the updated feature list:\n",
      "\n",
      "1.  `NO_REGIAO`\n",
      "2.  `TP_DEPENDENCIA`\n",
      "3.  `TP_LOCALIZACAO`\n",
      "4.  `IN_BIBLIOTECA`\n",
      "5.  `IN_LABORATORIO_INFORMATICA`\n",
      "6.  `IN_QUADRA_ESPORTES`\n",
      "7.  `IN_REFEITORIO`\n",
      "8.  `IN_COMPUTADOR`\n",
      "9.  `IN_BANDA_LARGA`\n",
      "10. `QT_PROF_ADMINISTRATIVOS`\n",
      "11. `IN_ALIMENTACAO`\n",
      "12. `QT_MAT_BAS`\n",
      "13. `IN_INTERNET` (Target)\n",
      "\n",
      "Now, I'll prepare the JSON output with these corrected features.\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"file_name\": \"DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv\",\n",
      "  \"header_row\": 0,\n",
      "  \"use_columns\": [\n",
      "    \"NO_REGIAO\",\n",
      "    \"TP_DEPENDENCIA\",\n",
      "    \"TP_LOCALIZACAO\",\n",
      "    \"IN_BIBLIOTECA\",\n",
      "    \"IN_LABORATORIO_INFORMATICA\",\n",
      "    \"IN_QUADRA_ESPORTES\",\n",
      "    \"IN_REFEITORIO\",\n",
      "    \"IN_COMPUTADOR\",\n",
      "    \"IN_BANDA_LARGA\",\n",
      "    \"QT_PROF_ADMINISTRATIVOS\",\n",
      "    \"IN_ALIMENTACAO\",\n",
      "    \"QT_MAT_BAS\",\n",
      "    \"IN_INTERNET\"\n",
      "  ],\n",
      "  \"delimiter\": \";\"\n",
      "}\n",
      "```\n",
      "\n",
      ">> CritiqueAgent is thinking...\n",
      "   ```json\n",
      "{\n",
      "\"decision\": \"REVISE DATA ENGINEERING\",\n",
      "\"reason\": \"Feature selection seems inadequate.\"\n",
      "}\n",
      "```\n",
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   Okay, the critique agent *again* signals \"REVISE DATA ENGINEERING\". This suggests there's a fundamental issue with my feature selection strategy. I need to move beyond just infrastructure and basic resources. Perhaps the model needs more information about the students and staff at the school.\n",
      "\n",
      "Let's try including features related to the teaching staff, student demographics, and specific educational resources.\n",
      "\n",
      "Revised Feature List:\n",
      "\n",
      "1.  `NO_REGIAO`: Region of the school.\n",
      "2.  `TP_DEPENDENCIA`: Type of administrative dependence.\n",
      "3.  `TP_LOCALIZACAO`: Type of location.\n",
      "4.  `IN_BIBLIOTECA`: Indicates library presence.\n",
      "5.  `IN_LABORATORIO_INFORMATICA`: Computer lab indicator.\n",
      "6.  `IN_BANDA_LARGA`: Broadband access indicator.\n",
      "7.  `QT_DOC_BAS`: Number of basic education teachers.\n",
      "8.  `QT_MAT_BAS`: Total number of basic education students.\n",
      "9.  `IN_ALIMENTACAO`: Indicates if the school provides meals.\n",
      "10. `IN_MATERIAL_PED_MULTIMIDIA`: Indicates multimedia teaching materials.\n",
      "11. `IN_MATERIAL_PED_INFANTIL`: Indicates teaching materials for children.\n",
      "12. `IN_EJA`: Indicates if the school offers youth and adult education.\n",
      "13. `IN_INTERNET`: Target variable.\n",
      "\n",
      "Here's the JSON output for the `load_dataset` function with this revised feature set:\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"file_name\": \"DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv\",\n",
      "  \"header_row\": 0,\n",
      "  \"use_columns\": [\n",
      "    \"NO_REGIAO\",\n",
      "    \"TP_DEPENDENCIA\",\n",
      "    \"TP_LOCALIZACAO\",\n",
      "    \"IN_BIBLIOTECA\",\n",
      "    \"IN_LABORATORIO_INFORMATICA\",\n",
      "    \"IN_BANDA_LARGA\",\n",
      "    \"QT_DOC_BAS\",\n",
      "    \"QT_MAT_BAS\",\n",
      "    \"IN_ALIMENTACAO\",\n",
      "    \"IN_MATERIAL_PED_MULTIMIDIA\",\n",
      "    \"IN_MATERIAL_PED_INFANTIL\",\n",
      "    \"IN_EJA\",\n",
      "    \"IN_INTERNET\"\n",
      "  ],\n",
      "  \"delimiter\": \";\"\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: load_dataset successful. Stored under key: df_microdados_ed_basica_2024 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call', 'function_call', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   I have successfully loaded the dataset. The `df_key` is `df_microdados_ed_basica_2024`.\n",
      "\n",
      "Now, I need to perform the crucial analysis and verification step using `dataset_info`, `preview_dataset`, and `describe_dataset` to understand the data.\n",
      "--- Tool: Describing dataset df_microdados_ed_basica_2024 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   Okay, I've obtained the dataset information, a preview, and descriptive statistics. Here are some key observations:\n",
      "\n",
      "*   **Target Variable:** `IN_INTERNET` has 181,065 non-null values. This means there are missing values. The mean is 0.92, indicating that a large proportion of schools in the dataset *do* have internet access.\n",
      "*   **Missing Values:** Several columns have missing values (NaNs), including `IN_BIBLIOTECA`, `IN_LABORATORIO_INFORMATICA`, `IN_BANDA_LARGA`, `IN_ALIMENTACAO`, `IN_MATERIAL_PED_MULTIMIDIA`, `IN_MATERIAL_PED_INFANTIL`, `IN_EJA`, `QT_MAT_BAS`, and `QT_DOC_BAS`.\n",
      "*   **Data Types:** Most of the indicator variables (starting with \"IN\\_\") are encoded as `float64`. `TP_DEPENDENCIA` and `TP_LOCALIZACAO` are `int64`, and `NO_REGIAO` is an `object`. The number of students and teachers (`QT_MAT_BAS`, `QT_DOC_BAS`) are also `float64`.\n",
      "\n",
      "Before proceeding, I need to address the missing values and convert the columns to appropriate datatypes.\n",
      "\n",
      "**Handling Missing Values**\n",
      "\n",
      "Given the relatively high number of rows (215545), I will use the `clean_dataset` tool to fill NaN values with the mean (for numeric columns with > 2 unique values) or mode (for categorical/boolean-like columns).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jean\\AppData\\Local\\Temp\\ipykernel_19860\\2522502380.py:300: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(fill_value, inplace=True)\n",
      "C:\\Users\\Jean\\AppData\\Local\\Temp\\ipykernel_19860\\2522502380.py:295: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[col].fillna(fill_value, inplace=True)\n",
      "Warning: there are non-text parts in the response: ['function_call', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   The `clean_dataset` tool has successfully filled the NaN values and removed duplicate rows. The number of rows has been reduced from 215545 to 148552.\n",
      "\n",
      "Now, I'll convert the appropriate columns to categorical and integer types.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   The columns have been successfully converted to the correct datatypes. `NO_REGIAO` is now categorical, and the indicator variables are integers.\n",
      "\n",
      "Next, I will split the data into features (X) and target (y). The target variable is `IN_INTERNET`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   The data has been successfully split into features (X) with key `X_df_microdados_ed_basica_2024` and target (y) with key `y_df_microdados_ed_basica_2024`.\n",
      "\n",
      "Since the target variable `IN_INTERNET` is binary (0 or 1), this is a classification problem. I will use `train_test_split_data_for_classifier` to split the data into training and testing sets, ensuring stratification.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   The data has been split into training and testing sets. Now it's time to train the XGBoost model. Since this is the first run, I will use the `train_xgboost_model` tool with default hyperparameters. The `model_type` is 'classifier'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   The XGBoost classifier model has been trained. The `model_key` is `xgb_classifier_model` and the `hyperparameters_used` are `{'colsample_bytree': 0.8, 'enable_categorical': True, 'max_depth': 3, 'n_estimators': 100, 'random_state': 42, 'subsample': 0.8}`.\n",
      "\n",
      "Now, I will evaluate the model's performance using the test set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   The model has been evaluated. The performance metrics are:\n",
      "\n",
      "*   F1-Score: 0.9775\n",
      "*   Precision: 0.9643\n",
      "*   Recall: 0.9911\n",
      "\n",
      "Finally, I will save the trained model and its metadata.\n",
      "--- Tool: Saved model to trained_model_artifacts\\xgb_classifier_model.json and metadata to trained_model_artifacts\\model_metadata.json ---\n",
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   I have saved the model and its metadata.\n",
      "\n",
      "The final output is the dictionary of performance metrics:\n",
      "```json\n",
      "{\n",
      "    \"F1-Score\": 0.97749991292535965,\n",
      "    \"Precision\": 0.96426730802267646,\n",
      "    \"Recall\": 0.991100752198326\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Tool Call] exit_loop activated by CritiqueAgent ---\n",
      "\n",
      ">> CritiqueAgent is thinking...\n",
      "   The F1-Score is 0.9775, which is greater than 0.8. Therefore, the model performance is successful and the loop should exit.\n",
      "\n",
      "--- PIPELINE FINISHED ---\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Verifica se a variável API Key do Gemini NÃO existe\n",
    "    if not GOOGLE_API_KEY:\n",
    "        # Se não existir, avisa e ENCERRA o programa\n",
    "        print(\"ERRO: A variável de ambiente GOOGLE_API_KEY não foi encontrada.\")\n",
    "        exit() # Encerra o script aqui\n",
    "\n",
    "    # Inicializa a pipeline\n",
    "    # asyncio.run(run_pipeline())   # para arquivos .py\n",
    "    await run_pipeline()            # para arquivos .ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
