{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e396bcaa",
   "metadata": {},
   "source": [
    "# ADK Gov AI Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddc8d2",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.5em; line-height: 1.3;\">\n",
    "Automação de Workflow de Data com Dados Públicos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1814f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ba55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jean\\Desktop\\ADK Gov AI Agents\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:198: UserWarning: Field name \"config_type\" in \"SequentialAgent\" shadows an attribute in parent \"BaseAgent\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "from google.adk.agents import LlmAgent, LoopAgent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from google.genai import types\n",
    "import google.genai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "from itertools import product\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import asyncio\n",
    "import pprint\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b6a1a",
   "metadata": {},
   "source": [
    "### === Definições Iniciais ==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec89c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes dos agentes\n",
    "APP_NAME = \"full_datascience_pipeline_app\"\n",
    "USER_ID = \"dev_user_01\"\n",
    "SESSION_ID = \"session_01\"\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"    # [ 'gemini-2.0-flash' | 'gemini-2.0-flash-lite' | 'gemini-2.5-flash' | 'gemini-2.5-flash-lite' ]\n",
    "\n",
    "# Outras constantes\n",
    "URL = \"https://download.inep.gov.br/dados_abertos/microdados_censo_escolar_2024.zip\"    # URL para download dos dados\n",
    "DATA_DIR = \"DATA\"                                                                       # diretório local onde os dados brutos serão baixados e extraídos\n",
    "TIME = 1                                                                                # pausa em segundos entre cada ação para evitar erros de \"quota exceeded\" da API (limite de requisições por minuto)\n",
    "MAX_ITERATIONS = 3                                                                      # máximo de loops da pipeline agêntica\n",
    "\n",
    "# Chaves de Estados da sessão\n",
    "DATA_WORKSPACE = {}\n",
    "STATE_ENGINEERING_SUMMARY = \"engineering_summary\"\n",
    "STATE_PERFORMANCE_METRICS = \"performance_metrics\"\n",
    "STATE_HYPERPARAMETERS = \"hyperparameters\"\n",
    "STATE_CRITIQUE = \"critique_output\"\n",
    "REENGINEER_SIGNAL = \"REVISE DATA ENGINEERING\"\n",
    "TUNE_HYPERPARAMETERS_SIGNAL = \"REVISE HYPERPARAMETER TUNING\"\n",
    "\n",
    "# Query do usuário\n",
    "INITIAL_QUERY = (\n",
    "    f\"Verifique os dados contidos na pasta '{DATA_DIR}' e encontre o arquivo principal referente às escolas. \"\n",
    "    \"Utilize os dicionários de dados dos datasets (se existirem). \"\n",
    "    \"O objetivo é prever se uma escola possui internet. \"\n",
    "#    \"O objetivo é prever se uma escola possui água potável. \"\n",
    "    \"Selecione colunas relevantes (como a localização e infraestrutura da escola) para construir o modelo.\"\n",
    "    \"Para o workflow, utilize somente as ferramentas previamente dadas.\"\n",
    ")\n",
    "\n",
    "# Carrega variáveis de ambiente\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_GENAI_USE_VERTEXAI = os.getenv(\"GOOGLE_GENAI_USE_VERTEXAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad58f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Modelos disponíveis que suportam 'generateContent':\")\n",
    "# print(\"-------------------------------------------------\")\n",
    "\n",
    "# client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# for model in client.models.list():\n",
    "#     print(f\"Nome da API: {model.name}\")\n",
    "#     print(f\"  Nome de Exibição: {model.display_name}\")\n",
    "#     print(f\"  Descrição: {model.description}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ccef6",
   "metadata": {},
   "source": [
    "### Dowload e Extração dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6694bb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o processo de download e extração...\n",
      "--- Garantindo que o diretório 'DATA' exista. ---\n",
      "--- O arquivo 'DATA\\microdados_censo_escolar_2024.zip' já existe. Pulando o download. ---\n",
      "--- O conteúdo já parece ter sido extraído em 'DATA'. Pulando a extração. ---\n",
      "\n",
      "Processo finalizado.\n",
      "Verifique a pasta 'DATA' para ver os resultados.\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract(url: str, data_dir: str):\n",
    "    \"\"\"\n",
    "    Baixa um arquivo de uma URL, salva-o em um diretório específico e o extrai.\n",
    "\n",
    "    A função verifica se o arquivo já foi baixado e se o conteúdo já foi\n",
    "    extraído antes de executar as operações, evitando trabalho redundante.\n",
    "\n",
    "    Args:\n",
    "        url: A URL do arquivo a ser baixado.\n",
    "        data_dir: O diretório para salvar o arquivo e extrair seu conteúdo.\n",
    "    \"\"\"\n",
    "    # Garante que o diretório de destino exista.\n",
    "    print(f\"--- Garantindo que o diretório '{data_dir}' exista. ---\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Define o caminho de salvamento do arquivo dentro do diretório de dados.\n",
    "    filename = os.path.basename(url)\n",
    "    archive_path = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Verifica se o arquivo já existe para evitar um novo download.\n",
    "    if not os.path.exists(archive_path):\n",
    "        print(f\"--- Baixando arquivo de {url} para {archive_path} ---\")\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            # Lança uma exceção para respostas com erro (ex: 404, 500).\n",
    "            response.raise_for_status()\n",
    "            with open(archive_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(\"--- Download concluído com sucesso. ---\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro ao baixar o arquivo: {e}\")\n",
    "            return # Interrompe a execução se o download falhar.\n",
    "    else:\n",
    "        print(f\"--- O arquivo '{archive_path}' já existe. Pulando o download. ---\")\n",
    "\n",
    "    # Verifica se o conteúdo já foi extraído.\n",
    "    # Esta verificação assume que o arquivo .zip contém uma pasta principal\n",
    "    # com o mesmo nome do arquivo (ex: 'ml-latest-small.zip' -> 'ml-latest-small/').\n",
    "    extracted_folder_name = os.path.splitext(filename)[0]\n",
    "    expected_extracted_path = os.path.join(data_dir, extracted_folder_name)\n",
    "\n",
    "    if os.path.exists(expected_extracted_path):\n",
    "        print(f\"--- O conteúdo já parece ter sido extraído em '{data_dir}'. Pulando a extração. ---\")\n",
    "    else:\n",
    "        # Extrai o arquivo baixado para o mesmo diretório.\n",
    "        print(f\"--- Extraindo {archive_path} para {data_dir} ---\")\n",
    "        try:\n",
    "            if archive_path.endswith('.zip'):\n",
    "                with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(data_dir)\n",
    "            elif archive_path.endswith(('.tar', '.tar.gz', '.tgz', '.tar.bz2')):\n",
    "                with tarfile.open(archive_path, 'r:*') as tar_ref:\n",
    "                    tar_ref.extractall(path=data_dir)\n",
    "            else:\n",
    "                print(f\"Formato de arquivo não suportado para extração: {archive_path}\")\n",
    "                return\n",
    "\n",
    "            print(f\"--- Arquivo extraído com sucesso para '{data_dir}'. ---\")\n",
    "\n",
    "        except (zipfile.BadZipFile, tarfile.ReadError) as e:\n",
    "            print(f\"Erro ao extrair o arquivo: {e}\")\n",
    "\n",
    "\n",
    "print(\"Iniciando o processo de download e extração...\")\n",
    "download_and_extract(url=URL, data_dir=DATA_DIR)\n",
    "print(\"\\nProcesso finalizado.\")\n",
    "print(f\"Verifique a pasta '{DATA_DIR}' para ver os resultados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730ff56",
   "metadata": {},
   "source": [
    "### Ferramentas (Tools) dos AI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02de112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_project_files(start_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Lists all folders, subfolders, and their files within a directory.\n",
    "\n",
    "    Args:\n",
    "        start_path: The directory to start listing from.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status and a string representing the file tree.\n",
    "    \"\"\"\n",
    "    if '..' in start_path:\n",
    "        return {\"status\": \"error\", \"message\": \"Path cannot contain '..'. Access is restricted.\"}\n",
    "    try:\n",
    "        tree_string = \"\"\n",
    "        for root, dirs, files in os.walk(start_path):\n",
    "            if any(d in root for d in ['__pycache__', '.venv', 'env', '.git']):\n",
    "                continue\n",
    "            level = root.replace(start_path, '').count(os.sep)\n",
    "            indent = \" \" * 4 * level\n",
    "            tree_string += f\"{indent}{os.path.basename(root)}/\\n\"\n",
    "            sub_indent = \" \" * 4 * (level + 1)\n",
    "            for f in files:\n",
    "                tree_string += f\"{sub_indent}{f}\\n\"\n",
    "        print(f\"--- Tool: Listing files in {start_path} ---\")\n",
    "        return {\"status\": \"success\", \"file_tree\": tree_string or \"No files or directories found.\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def read_text_file(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads the content of an unstructured text file (e.g., .txt, .md, README).\n",
    "    \n",
    "    This is useful for understanding the context or documentation associated with a dataset.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status and the content of the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        print(f\"--- Tool: Reading text file {file_path} ---\")\n",
    "        return {\"status\": \"success\", \"file_path\": file_path, \"content\": content}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Error reading file {file_path}: {e}\"}\n",
    "\n",
    "\n",
    "def inspect_file_structure(file_path: str, num_rows: int, header_row: int) -> dict:\n",
    "    \"\"\"\n",
    "    Previews the top of a file to identify its structure (header, columns, delimiter).\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the dataset file (CSV or Excel).\n",
    "        num_rows: The number of rows to preview. If None, defaults to 10.\n",
    "        header_row: The 0-indexed row presumed to be the header. If None, defaults to 0.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Internal Defaults ---\n",
    "        if num_rows is None:\n",
    "            num_rows = 10\n",
    "        if header_row is None:\n",
    "            header_row = 0\n",
    "\n",
    "        extension = os.path.splitext(file_path)[-1].lower()\n",
    "        preview_args = {'header': None, 'nrows': num_rows}\n",
    "        column_args = {'header': header_row}\n",
    "\n",
    "        if extension == '.csv':\n",
    "            df_preview = pd.read_csv(file_path, **preview_args, sep=None, engine='python', encoding='latin1')\n",
    "            df_cols = pd.read_csv(file_path, **column_args, nrows=0, encoding='latin1')\n",
    "        elif extension in ['.xls', '.xlsx']:\n",
    "            df_preview = pd.read_excel(file_path, **preview_args)\n",
    "            df_cols = pd.read_excel(file_path, **column_args)\n",
    "        else:\n",
    "            return {\"status\": \"error\", \"message\": f\"Unsupported file type: '{extension}'.\"}\n",
    "\n",
    "        print(f\"--- Tool: Inspecting file structure for {file_path} ---\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"file_path\": file_path,\n",
    "            \"header_preview\": df_preview.to_string(),\n",
    "            \"column_names\": df_cols.columns.tolist()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Error inspecting file {file_path}: {e}\"}\n",
    "    \n",
    "\n",
    "def read_data_dictionary(file_path: str, use_columns: List[str]) -> Dict[str, Union[str, int, List[str], List[Dict[str, Any]]]]:\n",
    "    \"\"\"\n",
    "    Reads specific columns from a CSV or Excel file, returning all rows.\n",
    "\n",
    "    This function is ideal for reading data dictionaries or other columnar data where\n",
    "    you need the complete information from a subset of columns.\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the CSV or Excel file.\n",
    "        use_columns: A list of column names to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the operation's status. On success, it includes the\n",
    "        retrieved data as a list of dictionaries, where each dictionary is a row.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(file_path):\n",
    "            return {\"status\": \"error\", \"message\": f\"File not found: {file_path}\"}\n",
    "        extension = os.path.splitext(file_path)[-1].lower()\n",
    "        if extension == '.csv':\n",
    "            df = pd.read_csv(file_path, usecols=use_columns, encoding='latin1')\n",
    "        elif extension in ['.xls', '.xlsx']:\n",
    "            df = pd.read_excel(file_path, usecols=use_columns)\n",
    "        else:\n",
    "            return {\"status\": \"error\", \"message\": f\"Unsupported file type: '{extension}'.\"}\n",
    "        \n",
    "        records = df.to_dict(orient='records')\n",
    "        print(f\"--- Tool: Reading data dictionary from {file_path} ---\")\n",
    "        return {\"status\": \"success\", \"file_path\": file_path, \"data\": records}\n",
    "    except ValueError as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Column error in {file_path}: {e}.\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Error reading {file_path}: {e}\"}\n",
    "\n",
    "\n",
    "def load_dataset(file_name: str, header_row: int, use_columns: List[str], delimiter: str) -> dict:\n",
    "    \"\"\"\n",
    "    Loads data from a CSV or Excel file into a DataFrame in the workspace.\n",
    "\n",
    "    Args:\n",
    "        file_name: The path of the file to load.\n",
    "        header_row: The 0-indexed row containing column names. If None, defaults to 0.\n",
    "        use_columns: A list of column names to load. If None, all columns are loaded.\n",
    "        delimiter: The character for separating values in a CSV. If None, defaults to ','.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # --- Internal Defaults ---\n",
    "        if header_row is None:\n",
    "            header_row = 0\n",
    "        if delimiter is None:\n",
    "            delimiter = ','\n",
    "\n",
    "        extension = os.path.splitext(file_name)[-1].lower()\n",
    "        if extension == '.csv':\n",
    "            df = pd.read_csv(file_name, header=header_row, usecols=use_columns, sep=delimiter,\n",
    "                             low_memory=False, encoding='latin1')\n",
    "        elif extension in ['.xls', '.xlsx']:\n",
    "            df = pd.read_excel(file_name, header=header_row, usecols=use_columns)\n",
    "        else:\n",
    "            return {\"status\": \"error\", \"message\": \"Invalid file type. Must be 'csv' or 'xlsx'.\"}\n",
    "\n",
    "        df_key = f\"df_{os.path.basename(file_name).split('.')[0]}\"\n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        print(f\"--- Tool: load_dataset successful. Stored under key: {df_key} ---\")\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"rows_loaded\": len(df),\n",
    "            \"columns_loaded\": len(df.columns),\n",
    "            \"columns\": df.columns.tolist()\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    " \n",
    "\n",
    "def preview_dataset(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Previews the first n rows of a DataFrame from the workspace.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame in the workspace.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and a string representation of the DataFrame's head.\n",
    "    \"\"\"\n",
    "    # Number of rows to be seen\n",
    "    n = 10\n",
    "\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    df = DATA_WORKSPACE[df_key]\n",
    "    return {\"status\": \"success\", \"df_key\": df_key, \"preview\": df.head(n).to_string()}\n",
    "\n",
    "\n",
    "def dataset_info(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Provides technical information about a DataFrame (columns, types, non-null counts).\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame in the workspace.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the DataFrame's info as a string.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    df = DATA_WORKSPACE[df_key]\n",
    "    buffer = io.StringIO()\n",
    "    df.info(buf=buffer)\n",
    "    return {\"status\": \"success\", \"df_key\": df_key, \"info\": buffer.getvalue()}\n",
    "\n",
    "\n",
    "def describe_dataset(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Provides descriptive statistics for a DataFrame in the workspace.\n",
    "    Includes statistics for both numeric and object/categorical columns.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame in the workspace.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status and a string representation of the described DataFrame.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    try:\n",
    "        df = DATA_WORKSPACE[df_key]\n",
    "        # include='all' ensures that statistics for all column types are generated.\n",
    "        description = df.describe(include='all').to_string()\n",
    "        print(f\"--- Tool: Describing dataset {df_key} ---\")\n",
    "        return {\"status\": \"success\", \"df_key\": df_key, \"description\": description}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Error describing DataFrame {df_key}: {e}\"}\n",
    "\n",
    "\n",
    "def clean_dataset(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Cleans a DataFrame by removing rows with NaN values and duplicates. Modifies in place.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame to clean.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and statistics about the cleaning process.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "\n",
    "    try:\n",
    "        df = DATA_WORKSPACE[df_key]\n",
    "        rows_before = len(df)\n",
    "        df.dropna(inplace=True)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        rows_after = len(df)\n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"rows_before\": rows_before,\n",
    "            \"rows_after\": rows_after,\n",
    "            \"rows_removed\": rows_before - rows_after\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def convert_to_categorical(df_key: str, columns_to_convert: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Converts specified columns in a DataFrame to the 'category' dtype. Modifies in place.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame to modify.\n",
    "        columns_to_convert: A list of column names to convert.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary confirming the status and listing the converted columns.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    try:\n",
    "        df = DATA_WORKSPACE[df_key]\n",
    "        converted = []\n",
    "        not_found = []\n",
    "        for col in columns_to_convert:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype('category')\n",
    "                converted.append(col)\n",
    "            else:\n",
    "                not_found.append(col)\n",
    "        \n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"converted_columns\": converted,\n",
    "            \"columns_not_found\": not_found\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "\n",
    "def convert_to_int(df_key: str, columns_to_convert: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Converts specified columns in a DataFrame to the integer dtype. Modifies in place.\n",
    "\n",
    "    Note: This will fail if a column contains non-numeric values or NaNs.\n",
    "    The calling code should handle such potential errors.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame to modify in DATA_WORKSPACE.\n",
    "        columns_to_convert: A list of column names to convert.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary confirming the status and listing the converted columns.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "\n",
    "    try:\n",
    "        df = DATA_WORKSPACE[df_key]\n",
    "        converted = []\n",
    "        not_found = []\n",
    "        \n",
    "        for col in columns_to_convert:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(int)\n",
    "                converted.append(col)\n",
    "            else:\n",
    "                not_found.append(col)\n",
    "        \n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"converted_columns\": converted,\n",
    "            \"columns_not_found\": not_found\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # This will catch errors like trying to convert a column with NaN or non-numeric strings\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def split_features_target(df_key: str, target_column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into features (X) and target (y). Stores them in the workspace.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame to split.\n",
    "        target_column: The name of the target column (y).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the new keys for features (X) and target (y).\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    df = DATA_WORKSPACE[df_key]\n",
    "    if target_column not in df.columns:\n",
    "        return {\"status\": \"error\", \"message\": f\"Target column '{target_column}' not in DataFrame.\"}\n",
    "    \n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    X_key = f\"X_{df_key}\"\n",
    "    y_key = f\"y_{df_key}\"\n",
    "    \n",
    "    DATA_WORKSPACE[X_key] = X\n",
    "    DATA_WORKSPACE[y_key] = y\n",
    "    \n",
    "    return {\"status\": \"success\", \"features_key\": X_key, \"target_key\": y_key}\n",
    "\n",
    "\n",
    "def train_test_split_data_for_classifier(X_key: str, y_key: str, test_size: float, random_state: int) -> dict:\n",
    "    \"\"\"\n",
    "    Splits features (X) and target (y) into training and testing sets.\n",
    "    Automatically sets the `stratify` parameter to y, so that the target is not unbalanced.\n",
    "\n",
    "    Args:\n",
    "        X_key: The workspace key for the features DataFrame (X).\n",
    "        y_key: The workspace key for the target Series (y).\n",
    "        test_size: Proportion for the test split.\n",
    "        random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the keys for X_train, X_test, y_train, and y_test.\n",
    "    \"\"\"\n",
    "    if X_key not in DATA_WORKSPACE or y_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"Feature key '{X_key}' or target key '{y_key}' not found.\"}\n",
    "        \n",
    "    X = DATA_WORKSPACE[X_key]\n",
    "    y = DATA_WORKSPACE[y_key]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=random_state,\n",
    "                                                        stratify=y) # stratify para que o target não seja desbalanceado\n",
    "    \n",
    "    keys = {\n",
    "        \"X_train\": f\"{X_key}_train\", \"X_test\": f\"{X_key}_test\",\n",
    "        \"y_train\": f\"{y_key}_train\", \"y_test\": f\"{y_key}_test\"\n",
    "    }\n",
    "    \n",
    "    DATA_WORKSPACE[keys[\"X_train\"]] = X_train\n",
    "    DATA_WORKSPACE[keys[\"X_test\"]] = X_test\n",
    "    DATA_WORKSPACE[keys[\"y_train\"]] = y_train\n",
    "    DATA_WORKSPACE[keys[\"y_test\"]] = y_test\n",
    "    \n",
    "    return {\"status\": \"success\", \"data_keys\": keys}\n",
    "\n",
    "\n",
    "def train_test_split_data_for_regressor(X_key: str, y_key: str, test_size: float, random_state: int) -> dict:\n",
    "    \"\"\"\n",
    "    Splits features (X) and target (y) into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        X_key: The workspace key for the features DataFrame (X).\n",
    "        y_key: The workspace key for the target Series (y).\n",
    "        test_size: Proportion for the test split.\n",
    "        random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the keys for X_train, X_test, y_train, and y_test.\n",
    "    \"\"\"\n",
    "    if X_key not in DATA_WORKSPACE or y_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"Feature key '{X_key}' or target key '{y_key}' not found.\"}\n",
    "        \n",
    "    X = DATA_WORKSPACE[X_key]\n",
    "    y = DATA_WORKSPACE[y_key]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                        test_size=test_size,\n",
    "                                                        random_state=random_state)\n",
    "    \n",
    "    keys = {\n",
    "        \"X_train\": f\"{X_key}_train\", \"X_test\": f\"{X_key}_test\",\n",
    "        \"y_train\": f\"{y_key}_train\", \"y_test\": f\"{y_key}_test\"\n",
    "    }\n",
    "    \n",
    "    DATA_WORKSPACE[keys[\"X_train\"]] = X_train\n",
    "    DATA_WORKSPACE[keys[\"X_test\"]] = X_test\n",
    "    DATA_WORKSPACE[keys[\"y_train\"]] = y_train\n",
    "    DATA_WORKSPACE[keys[\"y_test\"]] = y_test\n",
    "    \n",
    "    return {\"status\": \"success\", \"data_keys\": keys}\n",
    "    \n",
    "\n",
    "def train_xgboost_model(X_train_key: str, y_train_key: str, model_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Trains an XGBoost model (Classifier or Regressor) with a fixed set of\n",
    "    hyperparameters and stores it in the workspace.\n",
    "\n",
    "    Args:\n",
    "        X_train_key: The workspace key for the training features (X_train).\n",
    "        y_train_key: The workspace key for the training target (y_train).\n",
    "        model_type: The type of model to train, either 'classifier' or 'regressor'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the status, the key for the trained model, and the\n",
    "        hyperparameters that were used.\n",
    "    \"\"\"\n",
    "    # Check if the specified training data exists in the workspace\n",
    "    if X_train_key not in DATA_WORKSPACE or y_train_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Training data keys not found in workspace.\"}\n",
    "\n",
    "    try:\n",
    "        X_train = DATA_WORKSPACE[X_train_key]\n",
    "        y_train = DATA_WORKSPACE[y_train_key]\n",
    "\n",
    "        # Define the fixed set of hyperparameters\n",
    "        params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'enable_categorical': True,\n",
    "            'random_state': 42 # Added for reproducibility\n",
    "        }\n",
    "\n",
    "        # Select and instantiate the model based on the model_type parameter\n",
    "        if model_type.lower() == 'classifier':\n",
    "            model = XGBClassifier(**params)\n",
    "            model_key = \"xgb_classifier_model\"\n",
    "        elif model_type.lower() == 'regressor':\n",
    "            model = XGBRegressor(**params)\n",
    "            model_key = \"xgb_regressor_model\"\n",
    "        else:\n",
    "            return {\n",
    "                \"status\": \"error\",\n",
    "                \"message\": \"Invalid model_type. Please choose 'classifier' or 'regressor'.\"\n",
    "            }\n",
    "\n",
    "        # Train the selected model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Store the trained model in the workspace\n",
    "        DATA_WORKSPACE[model_key] = model\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"model_key\": model_key,\n",
    "            \"hyperparameters_used\": params\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def evaluate_classifier_performance(model_key: str, X_test_key: str, y_test_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a classifier model using precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        model_key: The workspace key of the trained classifier model.\n",
    "        X_test_key: The workspace key of the test features (X_test).\n",
    "        y_test_key: The workspace key of the true test target values (y_test).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the status and performance metrics.\n",
    "    \"\"\"\n",
    "    if model_key not in DATA_WORKSPACE or X_test_key not in DATA_WORKSPACE or y_test_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Model or test data keys not found in workspace.\"}\n",
    "        \n",
    "    try:\n",
    "        model = DATA_WORKSPACE[model_key]\n",
    "        X_test = DATA_WORKSPACE[X_test_key]\n",
    "        y_test = DATA_WORKSPACE[y_test_key]\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        precision = precision_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"metrics\": {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to evaluate classifier: {e}\"}\n",
    "\n",
    "\n",
    "def evaluate_regressor_performance(model_key: str, X_test_key: str, y_test_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a regressor model using MAE, RMSE, and R-squared.\n",
    "\n",
    "    Args:\n",
    "        model_key: The workspace key of the trained regressor model.\n",
    "        X_test_key: The workspace key of the test features (X_test).\n",
    "        y_test_key: The workspace key of the true test target values (y_test).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the status and performance metrics.\n",
    "    \"\"\"\n",
    "    if model_key not in DATA_WORKSPACE or X_test_key not in DATA_WORKSPACE or y_test_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Model or test data keys not found in workspace.\"}\n",
    "\n",
    "    try:\n",
    "        model = DATA_WORKSPACE[model_key]\n",
    "        X_test = DATA_WORKSPACE[X_test_key]\n",
    "        y_test = DATA_WORKSPACE[y_test_key]\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"metrics\": {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to evaluate regressor: {e}\"}\n",
    "\n",
    "\n",
    "def hyperparameter_search_xgboost(X_train_key: str, y_train_key: str, model_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Performs a hyperparameter grid search for an XGBoost model using cross-validation.\n",
    "\n",
    "    Args:\n",
    "        X_train_key: The workspace key for the training features (X_train).\n",
    "        y_train_key: The workspace key for the training target (y_train).\n",
    "        model_type: The type of model to tune, either 'classifier' or 'regressor'.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status, the key for the best model found, and the best parameters.\n",
    "    \"\"\"\n",
    "    if X_train_key not in DATA_WORKSPACE or y_train_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Training data keys not found in workspace.\"}\n",
    "    if model_type not in ['classifier', 'regressor']:\n",
    "        return {\"status\": \"error\", \"message\": \"model_type must be 'classifier' or 'regressor'.\"}\n",
    "\n",
    "    try:\n",
    "        X_train = DATA_WORKSPACE[X_train_key]\n",
    "        y_train = DATA_WORKSPACE[y_train_key]\n",
    "\n",
    "        # Convert data to DMatrix for XGBoost efficiency\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n",
    "\n",
    "        # Define hyperparameter grid\n",
    "        param_grid = {\n",
    "            'max_depth': [3, 5, 7],\n",
    "            'eta': np.logspace(start=0.01, stop=0.3, num=5),\n",
    "            'subsample': [0.6, 0.8],\n",
    "            'colsample_bytree': [0.6, 0.8]\n",
    "        }\n",
    "        \n",
    "        # Configure search based on model type\n",
    "        if model_type == 'regressor':\n",
    "            objective = 'reg:squarederror'\n",
    "            eval_metric = 'rmse'\n",
    "        else: # classifier\n",
    "            if y_train.nunique() == 2:\n",
    "                objective = 'binary:logistic'\n",
    "                eval_metric = 'logloss'\n",
    "            else:\n",
    "                objective = 'multi:softmax'\n",
    "                eval_metric = 'mlogloss'\n",
    "\n",
    "        # Placeholders for results\n",
    "        best_score = float(\"inf\")\n",
    "        best_params_combo = {}\n",
    "        \n",
    "        # Create all combinations for the grid search\n",
    "        search_space = list(product(*param_grid.values()))\n",
    "\n",
    "        print(f\"--- Tool: Starting XGBoost hyperparameter search for {model_type} ({len(search_space)} combinations) ---\")\n",
    "\n",
    "        for params_tuple in search_space:\n",
    "            params = dict(zip(param_grid.keys(), params_tuple))\n",
    "            params['objective'] = objective\n",
    "            \n",
    "            if model_type == 'classifier' and y_train.nunique() > 2:\n",
    "                 params['num_class'] = y_train.nunique()\n",
    "\n",
    "            # Execute cross-validation\n",
    "            cv_results = xgb.cv(\n",
    "                params=params,\n",
    "                dtrain=dtrain,\n",
    "                num_boost_round=500,\n",
    "                nfold=3,\n",
    "                metrics=eval_metric,\n",
    "                early_stopping_rounds=25,\n",
    "                verbose_eval=False\n",
    "            )\n",
    "            \n",
    "            # Get the best score from this CV run (lowest error)\n",
    "            current_score = cv_results[f'test-{eval_metric}-mean'].min()\n",
    "            \n",
    "            # Update best score and params if current run is better\n",
    "            if current_score < best_score:\n",
    "                best_score = current_score\n",
    "                best_params_combo = params\n",
    "                # Find the optimal number of boosting rounds\n",
    "                best_iteration = cv_results[f'test-{eval_metric}-mean'].idxmin()\n",
    "                best_params_combo['n_estimators'] = best_iteration\n",
    "\n",
    "        # Train the final model with the absolute best parameters\n",
    "        final_model_params = {k: v for k, v in best_params_combo.items() if k not in ['objective', 'num_class']}\n",
    "        \n",
    "        if model_type == 'regressor':\n",
    "            model = XGBRegressor(**final_model_params, objective=objective, enable_categorical=True)\n",
    "        else:\n",
    "            model = XGBClassifier(**final_model_params, objective=objective, enable_categorical=True)\n",
    "            \n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        model_key = f\"xgb_{model_type}_tuned_model\"\n",
    "        DATA_WORKSPACE[model_key] = model\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\", \n",
    "            \"model_key\": model_key,\n",
    "            \"best_params_found\": best_params_combo\n",
    "        }\n",
    "\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Hyperparameter search failed: {e}\"}\n",
    "\n",
    "\n",
    "def save_model_and_metadata(model_key: str, X_train_key: str, hyperparameters: Dict[str, Any], model_type: str) -> dict:\n",
    "    \"\"\"\n",
    "    Saves the trained model and its metadata (columns, hyperparameters).\n",
    "    The output folder is set internally to 'trained_model_artifacts'.\n",
    "\n",
    "    Args:\n",
    "        model_key: The workspace key for the trained model.\n",
    "        X_train_key: The workspace key for the training features (X_train).\n",
    "        hyperparameters: A dictionary of hyperparameters used for training.\n",
    "        model_type: The type of model ('classifier' or 'regressor').\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status and paths to the saved files.\n",
    "    \"\"\"\n",
    "    # --- Internal Defaults ---\n",
    "    output_folder = \"trained_model_artifacts\"\n",
    "\n",
    "    if model_key not in DATA_WORKSPACE or X_train_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Model or training data key not found in workspace.\"}\n",
    "\n",
    "    try:\n",
    "        # Create the output directory if it doesn't already exist\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "        # 1. Save the XGBoost model using its native method\n",
    "        model = DATA_WORKSPACE[model_key]\n",
    "        model_path = os.path.join(output_folder, f\"{model_key}.json\")\n",
    "        model.save_model(model_path)\n",
    "\n",
    "        # 2. Prepare and save the metadata\n",
    "        X_train = DATA_WORKSPACE[X_train_key]\n",
    "        # Ensure all hyperparameter values are JSON serializable\n",
    "        serializable_hyperparameters = {k: (v.item() if hasattr(v, 'item') else v) for k, v in hyperparameters.items()}\n",
    "        \n",
    "        metadata = {\n",
    "            \"model_type\": model_type,\n",
    "            \"feature_columns\": X_train.columns.tolist(),\n",
    "            \"hyperparameters\": serializable_hyperparameters,\n",
    "        }\n",
    "        metadata_path = os.path.join(output_folder, \"model_metadata.json\")\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "\n",
    "        print(f\"--- Tool: Saved model to {model_path} and metadata to {metadata_path} ---\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"model_path\": model_path,\n",
    "            \"metadata_path\": metadata_path\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def exit_loop(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"\n",
    "    Signals the main agent loop to stop iterating.\n",
    "\n",
    "    Args:\n",
    "        tool_context: The context object provided by the ADK framework.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary confirming that the exit signal has been sent.\n",
    "    \"\"\"\n",
    "    print(f\"--- [Tool Call] exit_loop activated by {tool_context.agent_name} ---\")\n",
    "    tool_context.actions.escalate = True\n",
    "    return {\"status\": \"success\", \"message\": \"Exit signal sent to the main loop.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda8d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista das ferramentas disponíveis para o agente de Data Engineering\n",
    "ENGINEERING_TOOLS = [list_project_files, load_dataset, inspect_file_structure, read_data_dictionary, read_text_file]\n",
    "\n",
    "# Lista das ferramentas disponíveis para o agente de Data Science\n",
    "SCIENCE_TOOLS = [load_dataset, dataset_info, describe_dataset, preview_dataset, clean_dataset, convert_to_categorical, convert_to_int, \n",
    "                 split_features_target, train_test_split_data_for_classifier, train_test_split_data_for_regressor, train_xgboost_model,\n",
    "                 evaluate_classifier_performance, evaluate_regressor_performance, hyperparameter_search_xgboost, save_model_and_metadata]\n",
    "\n",
    "# Lista das ferramentas disponíveis para o agente de Avaliação\n",
    "CRITIQUE_TOOLS = [exit_loop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb1a05",
   "metadata": {},
   "source": [
    "### Definição dos Agentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b818ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. O Agente \"Engenheiro de Dados\"\n",
    "data_engineer_agent = LlmAgent(\n",
    "    name=\"DataEngineerAgent\",\n",
    "    model=GEMINI_MODEL,\n",
    "    instruction=f\"\"\"\n",
    "    You are a highly efficient Data Engineer AI. Your goal is to logically identify and prepare the features for a machine learning model. You must follow these steps in order:\n",
    "\n",
    "    **1. Explore Files:**\n",
    "       - Use the `list_project_files` tool to see the available files. Identify the primary dataset and any potential data dictionaries (e.g., README, .md, .txt, or another .csv).\n",
    "\n",
    "    **2. Inspect Data Structure:**\n",
    "        - Use the `inspect_file_structure` tool on the primary data file. This will show you a preview of the data and give you the exact list of available column names.\n",
    "          From the preview, determine the correct `header_row` and `delimiter`. Note: Brazilian datasets often use a semicolon (';').\n",
    "\n",
    "    **3. Understand Column Meanings (If Necessary):**\n",
    "        - If there is a data dictionary or README file, use `read_text_file` (for unstructured text) or `read_data_dictionary` (for structured CSV/Excel files) to understand what each column represents.\n",
    "\n",
    "    **4. Select Features:**\n",
    "        - The session state key '{STATE_CRITIQUE}' may contain feedback. If it contains the signal \"{REENGINEER_SIGNAL}\", you **MUST** choose a **different combination of features**.\n",
    "          It could be more or less columns than chosen before.\n",
    "        - Based on the column names from the inspection step and your understanding of their meaning, select more or less 10 relevant columns (if possible), including the target variable.\n",
    "          **Do not guess column names; use only the names provided by the `inspect_file_structure` tool.**\n",
    "\n",
    "    **5. Final Output:**\n",
    "        - Your final output for this turn **MUST** be a single, valid JSON object that the next agent will use to call the `load_dataset` function.\n",
    "          It must contain the keys `file_name`, `header_row`, `use_columns`, and optionally `delimiter`.\n",
    "        - Example: {{\"file_name\": \"path/to/data.csv\", \"header_row\": 7, \"use_columns\": [\"col1\", \"col2\", \"col3\"], \"delimiter\": \";\"}}\n",
    "    \"\"\",\n",
    "    tools=ENGINEERING_TOOLS,\n",
    "    output_key=STATE_ENGINEERING_SUMMARY\n",
    ")\n",
    "\n",
    "# 2. O Agente \"Cientista de Dados\"\n",
    "data_scientist_agent = LlmAgent(\n",
    "    name=\"DataScientistAgent\",\n",
    "    model=GEMINI_MODEL,\n",
    "    instruction=f\"\"\"\n",
    "    You are a methodical Data Scientist AI. Your task is to preprocess data, train a model, evaluate it, and save the final artifacts. You must follow these steps in order:\n",
    "\n",
    "    **1. Load Data & Initial Analysis:**\n",
    "        - Get the data loading parameters from the session state key '{STATE_ENGINEERING_SUMMARY}'.\n",
    "        - Call the `load_dataset` tool. This returns a dictionary containing the `df_key`, which you **MUST** use to reference the dataset in all subsequent steps.\n",
    "        - **Crucial Analysis & Verification Step:** After loading, use `dataset_info`, `preview_dataset`, and `describe_dataset` on the `df_key` to understand the data's structure, content, and distribution before proceeding.\n",
    "\n",
    "    **2. Preprocess and Split:**\n",
    "        - Use the `clean_dataset` tool on the `df_key`.\n",
    "        - Use `convert_to_categorical` on the `df_key` for columns that are not numerical.\n",
    "        - Use `convert_to_int` on the `df_key` for columns that are supposed to be integers and NOT floats (e.g.: 0.0 and 1.0).\n",
    "        - Use `split_features_target` to separate features (X) and target (y).\n",
    "        - Based on the target variable, use either `train_test_split_data_for_classifier` or `train_test_split_data_for_regressor`.\n",
    "\n",
    "    **3. Train Model:**\n",
    "        - **Decision Point:** Check the session state key '{STATE_CRITIQUE}' for a decision from the previous loop.\n",
    "        - **If the decision was \"{TUNE_HYPERPARAMETERS_SIGNAL}\":** You **MUST** call the `hyperparameter_search_xgboost` tool. Determine the `model_type` ('classifier' or 'regressor')\n",
    "          based on the split function you used.\n",
    "        - **Otherwise (first run):** You **MUST** call the `train_xgboost_model` tool. Determine the `model_type` ('classifier' or 'regressor') based on which split function you used\n",
    "          in the previous step. This tool uses a fixed set of default hyperparameters internally.\n",
    "        - **Crucially, you MUST capture the output of this step, as it contains the `model_key` and the `hyperparameters_used` or `best_params_found` needed for the next steps.**\n",
    "\n",
    "    **4. Evaluate Model:**\n",
    "        - Use the `model_key` from the previous step to call either `evaluate_classifier_performance` or `evaluate_regressor_performance`.\n",
    "        - Capture the resulting performance metrics dictionary.\n",
    "        \n",
    "    **5. Save Artifacts:**\n",
    "        - After training and evaluation, you **MUST** save the results by calling the `save_model_and_metadata` tool.\n",
    "        - Provide the following arguments:\n",
    "            - `model_key`: The key of the model you just trained.\n",
    "            - `X_train_key`: The key for the training features (e.g., 'X_df_matriz_train').\n",
    "            - `hyperparameters`: The dictionary of hyperparameters you captured from the training step (either `hyperparameters_used` or `best_params_found`).\n",
    "            - `model_type`: The type of model you trained ('classifier' or 'regressor').\n",
    "\n",
    "    **6. Final Output:**\n",
    "        - Your final output for this turn **MUST** be the complete dictionary of performance metrics returned by the evaluation tool in Step 4.\n",
    "    \"\"\",\n",
    "    tools=SCIENCE_TOOLS,\n",
    "    output_key=STATE_PERFORMANCE_METRICS\n",
    ")\n",
    "\n",
    "# 3. O Agente \"Avaliador\"\n",
    "critique_agent = LlmAgent(\n",
    "    name=\"CritiqueAgent\",\n",
    "    model=GEMINI_MODEL,\n",
    "    instruction=f\"\"\"\n",
    "    You are a decisive Machine Learning Model Critic. Your role is to analyze model performance and determine the next action with structured output. You must follow these steps in order:\n",
    "\n",
    "    **1. Review Performance:**\n",
    "        - Analyze the performance dictionary from the session state key '{STATE_PERFORMANCE_METRICS}'.\n",
    "        - First, identify the primary metric (e.g., 'F1-Score', 'R-squared').\n",
    "        - Apply the following logic to make your decision:\n",
    "          - if score >= 0.85: Success!\n",
    "          - elif 0.85 > score >= 0.50: Moderate performance. Trigger hyperparameter re-tuning.\n",
    "          - elif score < 0.50: Poor performance. Signal feature re-engineering.\n",
    "\n",
    "    **2. Take Action:**\n",
    "        - **In the SUCCESS case:** Your ONLY action is to call the `exit_loop` tool. Do not output any JSON.\n",
    "        - **For re-tuning:** Your output MUST be a single JSON object: {{\"decision\": \"{TUNE_HYPERPARAMETERS_SIGNAL}\", \"reason\": \"Performance is moderate, initiating a full hyperparameter search.\"}}\n",
    "        - **For re-engineering:** Your output MUST be a single JSON object: {{\"decision\": \"{REENGINEER_SIGNAL}\", \"reason\": \"Feature selection seems inadequate.\"}}\n",
    "    \"\"\",\n",
    "    tools=CRITIQUE_TOOLS,\n",
    "    output_key=STATE_CRITIQUE\n",
    ")\n",
    "\n",
    "# 4. O Agente \"Orquestrador\"\n",
    "main_pipeline_agent = LoopAgent(\n",
    "    name=\"MainPipelineAgent\",\n",
    "    sub_agents=[\n",
    "        data_engineer_agent,\n",
    "        data_scientist_agent,\n",
    "        critique_agent\n",
    "    ],\n",
    "    max_iterations=MAX_ITERATIONS # Limite de loops da pipeline\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6717cd",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce24a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_pipeline():\n",
    "    \"\"\"Configures and runs the complete agent pipeline.\"\"\"\n",
    "    session_service = InMemorySessionService()\n",
    "    session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
    "    runner = Runner(agent=main_pipeline_agent, app_name=APP_NAME, session_service=session_service)\n",
    "\n",
    "    print(f\"--- STARTING PIPELINE WITH QUERY ---\\n'{INITIAL_QUERY}'\\n\")\n",
    "    content = types.Content(role='user', parts=[types.Part(text=INITIAL_QUERY)])\n",
    "\n",
    "    # The final answer will be accumulated here\n",
    "    final_response = \"\"\n",
    "    \n",
    "    try:\n",
    "        async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
    "            # Skip empty events\n",
    "            if not event.content or not event.content.parts:\n",
    "                continue\n",
    "\n",
    "            processed_tool_part = False\n",
    "\n",
    "            for part in event.content.parts:\n",
    "                # 1. Check for a tool call (code the agent wants to run)\n",
    "                if hasattr(part, 'executable_code') and part.executable_code:\n",
    "                    print(f\"\\n>> {event.author} is calling a tool:\")\n",
    "                    print(\"```python\")\n",
    "                    print(part.executable_code.code)\n",
    "                    print(\"```\")\n",
    "                    processed_tool_part = True\n",
    "\n",
    "                # 2. Check for the result of a tool call\n",
    "                elif hasattr(part, 'code_execution_result') and part.code_execution_result:\n",
    "                    output_str = pprint.pformat(part.code_execution_result.output)\n",
    "                    print(f\"\\n>> Tool result for {event.author}:\")\n",
    "                    print(output_str)\n",
    "                    processed_tool_part = True\n",
    "\n",
    "            # 3. If we haven't processed a tool part, any text is likely a \"thought\" or the final answer\n",
    "            if not processed_tool_part:\n",
    "                for part in event.content.parts:\n",
    "                    # It now safely checks if part.text exists and is not None before trying to use it.\n",
    "                    if hasattr(part, 'text') and part.text:\n",
    "                        text_content = part.text.strip()\n",
    "                        if text_content: # Process only if there is actual text after stripping\n",
    "                            if event.author == main_pipeline_agent.name:\n",
    "                                 final_response += part.text\n",
    "                            else:\n",
    "                                print(f\"\\n>> {event.author} is thinking...\\n   {text_content}\")\n",
    "            \n",
    "            # Pause for TIME seconds after processing each event to respect rate limits.\n",
    "            await asyncio.sleep(TIME)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- AN ERROR OCCURRED ---\\n\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n--- PIPELINE FINISHED ---\")\n",
    "    print(f\"\\nFinal Agent Response:\\n{final_response.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf5116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING PIPELINE WITH QUERY ---\n",
      "'Verifique os dados contidos na pasta 'DATA' e encontre o arquivo principal referente às escolas. Utilize os dicionários de dados dos datasets (se existirem). O objetivo é prever se uma escola possui internet. Selecione colunas relevantes (como a localização e infraestrutura da escola) para construir o modelo.Para o workflow, utilize somente as ferramentas previamente dadas.'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   Okay, let's start by exploring the files in the 'DATA' folder to identify the main dataset and any data dictionaries.\n",
      "--- Tool: Listing files in DATA ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   Okay, the file structure shows the main data file is likely `DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv`. There's also a data dictionary: `DATA/microdados_censo_escolar_2024/Anexos/ANEXO I - Dicionário de Dados/dicionário_dados_educação_básica.xlsx`. Let's inspect the structure of the main data file first.\n",
      "--- Tool: Inspecting file structure for DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   Okay, the delimiter is clearly a semicolon ';'. Now, let's examine the data dictionary to understand the columns better before selecting features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   It seems like the column names in the data dictionary are different. Let's try inspecting the file structure of the data dictionary to find the correct column names.\n",
      "--- Tool: Inspecting file structure for DATA/microdados_censo_escolar_2024/Anexos/ANEXO I - Dicionário de Dados/dicionário_dados_educação_básica.xlsx ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   It seems that the data dictionary is not in a usable format. Let's proceed with feature selection based on the column names from the main dataset inspection and their apparent meanings. The target variable is `IN_INTERNET`.\n",
      "\n",
      "Here are the columns I'll select:\n",
      "\n",
      "*   `IN_INTERNET` (Target variable)\n",
      "*   `TP_DEPENDENCIA` (Type of school administration)\n",
      "*   `TP_LOCALIZACAO` (Type of location - Urban/Rural)\n",
      "*   `CO_REGIAO` (Region code)\n",
      "*   `CO_UF` (State code)\n",
      "*   `CO_MUNICIPIO` (Municipality code)\n",
      "*   `IN_ENERGIA_REDE_PUBLICA` (Has public energy)\n",
      "*   `IN_AGUA_POTAVEL` (Has potable water)\n",
      "*   `IN_ESGOTO_REDE_PUBLICA` (Has public sewage system)\n",
      "*   `QT_SALAS_UTILIZADAS` (Number of used rooms)\n",
      "\n",
      "Now, let's create the JSON object for loading the dataset.\n",
      "--- Tool: load_dataset successful. Stored under key: df_microdados_ed_basica_2024 ---\n",
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   ```json\n",
      "{\"file_name\": \"DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv\", \"header_row\": 0, \"use_columns\": [\"IN_INTERNET\", \"TP_DEPENDENCIA\", \"TP_LOCALIZACAO\", \"CO_REGIAO\", \"CO_UF\", \"CO_MUNICIPIO\", \"IN_ENERGIA_REDE_PUBLICA\", \"IN_AGUA_POTAVEL\", \"IN_ESGOTO_REDE_PUBLICA\", \"QT_SALAS_UTILIZADAS\"], \"delimiter\": \";\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call', 'function_call', 'function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: Describing dataset df_microdados_ed_basica_2024 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   Okay, the data has been loaded, and I've performed initial analysis using `dataset_info`, `preview_dataset`, and `describe_dataset`.\n",
      "\n",
      "Now, let's preprocess the data. I'll start by cleaning the dataset to remove rows with NaN values.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   Now that I have cleaned the dataset, I will convert the categorical columns to the correct type.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   Now I will convert the columns that should be integers to the integer type.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   Now that the data is preprocessed, let's split the features (X) and target (y). The target variable is `IN_INTERNET`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   Now that I have the features and target variables, I will split the data into training and testing sets. Since the target variable is `IN_INTERNET`, which is a binary indicator, this is a classification problem.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   Now that the data is split into training and testing sets, I will train an XGBoost model. Since I have not received a 'REVISE HYPERPARAMETER TUNING' decision, I will train the model using the default hyperparameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   Now that the model is trained, I will evaluate its performance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   Now that the model has been trained and evaluated, I will save the model and its metadata.\n",
      "--- Tool: Saved model to trained_model_artifacts\\xgb_classifier_model.json and metadata to trained_model_artifacts\\model_metadata.json ---\n",
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   ```json\n",
      "{\n",
      "    \"F1-Score\": 0.9668877863872011,\n",
      "    \"Precision\": 0.9489550279023963,\n",
      "    \"Recall\": 0.9855113636363636\n",
      "}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- [Tool Call] exit_loop activated by CritiqueAgent ---\n",
      "\n",
      ">> CritiqueAgent is thinking...\n",
      "   The F1-Score is 0.967, which is greater than 0.85. This indicates successful model performance.\n",
      "\n",
      "--- PIPELINE FINISHED ---\n",
      "\n",
      "Final Agent Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Verifica se a variável API Key do Gemini NÃO existe\n",
    "    if not GOOGLE_API_KEY:\n",
    "        # Se não existir, avisa e ENCERRA o programa\n",
    "        print(\"ERRO: A variável de ambiente GOOGLE_API_KEY não foi encontrada.\")\n",
    "        exit() # Encerra o script aqui\n",
    "\n",
    "    # Inicializa a pipeline\n",
    "    # asyncio.run(run_pipeline())   # para arquivos .py\n",
    "    await run_pipeline()            # para arquivos .ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
