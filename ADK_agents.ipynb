{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e396bcaa",
   "metadata": {},
   "source": [
    "# ADK Gov AI Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ddc8d2",
   "metadata": {},
   "source": [
    "<p style=\"font-size: 1.5em; line-height: 1.3;\">\n",
    "Automação de Workflow de Data com Dados Públicos.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe1814f",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ba55cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jeanferrer/Desktop/ADK Gov AI Agent/.venv/lib/python3.10/site-packages/pydantic/_internal/_fields.py:198: UserWarning: Field name \"config_type\" in \"SequentialAgent\" shadows an attribute in parent \"BaseAgent\"\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "from google.adk.agents import LlmAgent, LoopAgent\n",
    "from google.adk.runners import Runner\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.tools.tool_context import ToolContext\n",
    "from google.genai import types\n",
    "import google.genai as genai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import asyncio\n",
    "import pprint\n",
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "import tarfile\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b6a1a",
   "metadata": {},
   "source": [
    "### $$$ Definições Iniciais $$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec89c36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constantes do Agente\n",
    "APP_NAME = \"full_datascience_pipeline_app\"\n",
    "USER_ID = \"dev_user_01\"\n",
    "SESSION_ID = \"session_01\"\n",
    "GEMINI_MODEL = \"gemini-2.0-flash\"    # gemini-2.0-flash | gemini-2.0-flash-exp | gemini-2.5-flash-lite\n",
    "\n",
    "# Outras constantes\n",
    "URL = \"https://download.inep.gov.br/dados_abertos/microdados_censo_escolar_2024.zip\"    # url para download dos dados\n",
    "DATA_DIR = \"DATA\"                                                                       # diretório local onde os dados brutos serão baixados e extraídos\n",
    "TIME = 1                                                                                # pausa em segundos para evitar erros de \"quota exceeded\" da API (limite de requisições por minuto)\n",
    "\n",
    "# Chaves de Estados da sessão\n",
    "DATA_WORKSPACE = {}\n",
    "STATE_ENGINEERING_SUMMARY = \"engineering_summary\"\n",
    "STATE_PERFORMANCE_METRICS = \"performance_metrics\"\n",
    "STATE_HYPERPARAMETERS = \"hyperparameters\"\n",
    "STATE_CRITIQUE = \"critique_output\"\n",
    "REENGINEER_SIGNAL = \"REVISAR ENGENHARIA DE DADOS\"\n",
    "EXIT_SIGNAL = \"PROCESSO CONCLUÍDO\"\n",
    "\n",
    "# Query do usuário\n",
    "INITIAL_QUERY = (\n",
    "    f\"Verifique os dados contidos na pasta '{DATA_DIR}' e encontre o arquivo principal referente às escolas. \"\n",
    "    \"Utilize os dicionários de dados dos datasets (se existirem). \"\n",
    "    \"O objetivo é prever se uma escola possui internet (`IN_INTERNET`). \"\n",
    "    \"Selecione colunas relevantes (como a localização e infraestrutura da escola) para construir o modelo.\"\n",
    "    \"Para o workflow, utilize somente as ferramentas previamente dadas.\"\n",
    ")\n",
    "\n",
    "# Carrega variáveis de ambiente\n",
    "load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "GOOGLE_GENAI_USE_VERTEXAI = os.getenv(\"GOOGLE_GENAI_USE_VERTEXAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad58f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Modelos disponíveis que suportam 'generateContent':\")\n",
    "# print(\"-------------------------------------------------\")\n",
    "\n",
    "# client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# for model in client.models.list():\n",
    "#     print(f\"Nome da API: {model.name}\")\n",
    "#     print(f\"  Nome de Exibição: {model.display_name}\")\n",
    "#     print(f\"  Descrição: {model.description}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683ccef6",
   "metadata": {},
   "source": [
    "### Dowload e Extração dos Dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6694bb48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando o processo de download e extração...\n",
      "--- Garantindo que o diretório 'DATA' exista. ---\n",
      "--- O arquivo 'DATA/microdados_censo_escolar_2024.zip' já existe. Pulando o download. ---\n",
      "--- O conteúdo já parece ter sido extraído em 'DATA'. Pulando a extração. ---\n",
      "\n",
      "Processo finalizado.\n",
      "Verifique a pasta 'DATA' para ver os resultados.\n"
     ]
    }
   ],
   "source": [
    "def download_and_extract(url: str, data_dir: str):\n",
    "    \"\"\"\n",
    "    Baixa um arquivo de uma URL, salva-o em um diretório específico e o extrai.\n",
    "\n",
    "    A função verifica se o arquivo já foi baixado e se o conteúdo já foi\n",
    "    extraído antes de executar as operações, evitando trabalho redundante.\n",
    "\n",
    "    Args:\n",
    "        url: A URL do arquivo a ser baixado.\n",
    "        data_dir: O diretório para salvar o arquivo e extrair seu conteúdo.\n",
    "    \"\"\"\n",
    "    # Garante que o diretório de destino exista.\n",
    "    print(f\"--- Garantindo que o diretório '{data_dir}' exista. ---\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "    # Define o caminho de salvamento do arquivo dentro do diretório de dados.\n",
    "    filename = os.path.basename(url)\n",
    "    archive_path = os.path.join(data_dir, filename)\n",
    "\n",
    "    # Verifica se o arquivo já existe para evitar um novo download.\n",
    "    if not os.path.exists(archive_path):\n",
    "        print(f\"--- Baixando arquivo de {url} para {archive_path} ---\")\n",
    "        try:\n",
    "            response = requests.get(url, stream=True)\n",
    "            # Lança uma exceção para respostas com erro (ex: 404, 500).\n",
    "            response.raise_for_status()\n",
    "            with open(archive_path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "            print(\"--- Download concluído com sucesso. ---\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro ao baixar o arquivo: {e}\")\n",
    "            return # Interrompe a execução se o download falhar.\n",
    "    else:\n",
    "        print(f\"--- O arquivo '{archive_path}' já existe. Pulando o download. ---\")\n",
    "\n",
    "    # Verifica se o conteúdo já foi extraído.\n",
    "    # Esta verificação assume que o arquivo .zip contém uma pasta principal\n",
    "    # com o mesmo nome do arquivo (ex: 'ml-latest-small.zip' -> 'ml-latest-small/').\n",
    "    extracted_folder_name = os.path.splitext(filename)[0]\n",
    "    expected_extracted_path = os.path.join(data_dir, extracted_folder_name)\n",
    "\n",
    "    if os.path.exists(expected_extracted_path):\n",
    "        print(f\"--- O conteúdo já parece ter sido extraído em '{data_dir}'. Pulando a extração. ---\")\n",
    "    else:\n",
    "        # Extrai o arquivo baixado para o mesmo diretório.\n",
    "        print(f\"--- Extraindo {archive_path} para {data_dir} ---\")\n",
    "        try:\n",
    "            if archive_path.endswith('.zip'):\n",
    "                with zipfile.ZipFile(archive_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(data_dir)\n",
    "            elif archive_path.endswith(('.tar', '.tar.gz', '.tgz', '.tar.bz2')):\n",
    "                with tarfile.open(archive_path, 'r:*') as tar_ref:\n",
    "                    tar_ref.extractall(path=data_dir)\n",
    "            else:\n",
    "                print(f\"Formato de arquivo não suportado para extração: {archive_path}\")\n",
    "                return\n",
    "\n",
    "            print(f\"--- Arquivo extraído com sucesso para '{data_dir}'. ---\")\n",
    "\n",
    "        except (zipfile.BadZipFile, tarfile.ReadError) as e:\n",
    "            print(f\"Erro ao extrair o arquivo: {e}\")\n",
    "\n",
    "\n",
    "print(\"Iniciando o processo de download e extração...\")\n",
    "download_and_extract(url=URL, data_dir=DATA_DIR)\n",
    "print(\"\\nProcesso finalizado.\")\n",
    "print(f\"Verifique a pasta '{DATA_DIR}' para ver os resultados.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6730ff56",
   "metadata": {},
   "source": [
    "### Ferramentas (Tools) dos AI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02de112f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_project_files(start_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Lists all folders, subfolders, and their files within a directory.\n",
    "\n",
    "    Args:\n",
    "        start_path: The directory to start listing from.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status and a string representing the file tree.\n",
    "    \"\"\"\n",
    "    if '..' in start_path:\n",
    "        return {\"status\": \"error\", \"message\": \"Path cannot contain '..'. Access is restricted.\"}\n",
    "\n",
    "    try:\n",
    "        tree_string = \"\"\n",
    "        for root, dirs, files in os.walk(start_path):\n",
    "            if any(d in root for d in ['__pycache__', '.venv', 'env', '.git']):\n",
    "                continue\n",
    "            level = root.replace(start_path, '').count(os.sep)\n",
    "            indent = \" \" * 4 * level\n",
    "            tree_string += f\"{indent}{os.path.basename(root)}/\\n\"\n",
    "            sub_indent = \" \" * 4 * (level + 1)\n",
    "            for f in files:\n",
    "                tree_string += f\"{sub_indent}{f}\\n\"\n",
    "\n",
    "        print(f\"--- Tool: Listing files in {start_path} ---\")\n",
    "        return {\"status\": \"success\", \"file_tree\": tree_string or \"No files or directories found.\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def read_text_file(file_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Reads the content of a simple text file (e.g., .txt, .md, .csv).\n",
    "\n",
    "    Args:\n",
    "        file_path: The path to the text file.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the status and the content of the file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            content = f.read()\n",
    "        print(f\"--- Tool: Reading text file {file_path} ---\")\n",
    "        return {\"status\": \"success\", \"file_path\": file_path, \"content\": content}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Error reading file {file_path}: {e}\"}\n",
    "\n",
    "\n",
    "def read_dataset(file_name: str, delimiter: str, use_columns: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Reads a dataset into the workspace or inspects its columns.\n",
    "    If 'use_columns' is omitted or empty, it only inspects and returns column names.\n",
    "\n",
    "    Args:\n",
    "        file_name: The path of the file to read.\n",
    "        delimiter: The character used to separate values.\n",
    "        use_columns: A list of column names to read. If empty/None, inspects columns.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the outcome. On success, includes the dataframe key and info.\n",
    "        On inspection, returns the list of columns. On error, returns an error message.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Inspection mode: returns only the column names\n",
    "        if not use_columns:\n",
    "            header_df = pd.read_csv(file_name, sep=delimiter, nrows=0, encoding='latin1', low_memory=False)\n",
    "            columns = header_df.columns.tolist()\n",
    "            print(f\"--- Tool: Inspected columns in {file_name} ---\")\n",
    "            return {\"status\": \"inspection_success\", \"columns\": columns, \"file_name\": file_name}\n",
    "\n",
    "        # Full read mode: loads the dataset into the workspace\n",
    "        df = pd.read_csv(file_name, sep=delimiter, usecols=use_columns, encoding='latin1', low_memory=False)\n",
    "        df_key = f\"df_{os.path.basename(file_name).split('.')[0]}\"\n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        print(f\"--- Tool: read_dataset successful. Stored under key: {df_key} ---\")\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"rows_loaded\": len(df),\n",
    "            \"columns_loaded\": len(df.columns)\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "    \n",
    "\n",
    "def preview_dataset(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Previews the first 5 rows of a DataFrame from the workspace.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame in the workspace.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and a string representation of the DataFrame's head.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    df = DATA_WORKSPACE[df_key]\n",
    "    return {\"status\": \"success\", \"df_key\": df_key, \"preview\": df.head().to_string()}\n",
    "\n",
    "\n",
    "def dataset_info(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Provides technical information about a DataFrame (columns, types, non-null counts).\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame in the workspace.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the DataFrame's info as a string.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    df = DATA_WORKSPACE[df_key]\n",
    "    buffer = io.StringIO()\n",
    "    df.info(buf=buffer)\n",
    "    return {\"status\": \"success\", \"df_key\": df_key, \"info\": buffer.getvalue()}\n",
    "\n",
    "\n",
    "def clean_dataset(df_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Cleans a DataFrame by removing rows with NaN values and duplicates. Modifies in place.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame to clean.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and statistics about the cleaning process.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "\n",
    "    try:\n",
    "        df = DATA_WORKSPACE[df_key]\n",
    "        rows_before = len(df)\n",
    "        df.dropna(inplace=True)\n",
    "        df.drop_duplicates(inplace=True)\n",
    "        rows_after = len(df)\n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"rows_before\": rows_before,\n",
    "            \"rows_after\": rows_after,\n",
    "            \"rows_removed\": rows_before - rows_after\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def convert_to_categorical(df_key: str, columns_to_convert: list[str]) -> dict:\n",
    "    \"\"\"\n",
    "    Converts specified columns in a DataFrame to the 'category' dtype. Modifies in place.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame to modify.\n",
    "        columns_to_convert: A list of column names to convert.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary confirming the status and listing the converted columns.\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    try:\n",
    "        df = DATA_WORKSPACE[df_key]\n",
    "        converted = []\n",
    "        not_found = []\n",
    "        for col in columns_to_convert:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype('category')\n",
    "                converted.append(col)\n",
    "            else:\n",
    "                not_found.append(col)\n",
    "        \n",
    "        DATA_WORKSPACE[df_key] = df\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"df_key\": df_key,\n",
    "            \"converted_columns\": converted,\n",
    "            \"columns_not_found\": not_found\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def split_features_target(df_key: str, target_column: str) -> dict:\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into features (X) and target (y). Stores them in the workspace.\n",
    "\n",
    "    Args:\n",
    "        df_key: The key of the DataFrame to split.\n",
    "        target_column: The name of the target column (y).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the new keys for features (X) and target (y).\n",
    "    \"\"\"\n",
    "    if df_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"DataFrame key '{df_key}' not found.\"}\n",
    "    \n",
    "    df = DATA_WORKSPACE[df_key]\n",
    "    if target_column not in df.columns:\n",
    "        return {\"status\": \"error\", \"message\": f\"Target column '{target_column}' not in DataFrame.\"}\n",
    "    \n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "    \n",
    "    X_key = f\"X_{df_key}\"\n",
    "    y_key = f\"y_{df_key}\"\n",
    "    \n",
    "    DATA_WORKSPACE[X_key] = X\n",
    "    DATA_WORKSPACE[y_key] = y\n",
    "    \n",
    "    return {\"status\": \"success\", \"features_key\": X_key, \"target_key\": y_key}\n",
    "\n",
    "\n",
    "def train_test_split_data(X_key: str, y_key: str, test_size: float, random_state: int) -> dict:\n",
    "    \"\"\"\n",
    "    Splits features (X) and target (y) into training and testing sets.\n",
    "\n",
    "    Args:\n",
    "        X_key: The workspace key for the features DataFrame (X).\n",
    "        y_key: The workspace key for the target Series (y).\n",
    "        test_size: Proportion for the test split.\n",
    "        random_state: Seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the keys for X_train, X_test, y_train, and y_test.\n",
    "    \"\"\"\n",
    "    if X_key not in DATA_WORKSPACE or y_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": f\"Feature key '{X_key}' or target key '{y_key}' not found.\"}\n",
    "        \n",
    "    X = DATA_WORKSPACE[X_key]\n",
    "    y = DATA_WORKSPACE[y_key]\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    keys = {\n",
    "        \"X_train\": f\"{X_key}_train\", \"X_test\": f\"{X_key}_test\",\n",
    "        \"y_train\": f\"{y_key}_train\", \"y_test\": f\"{y_key}_test\"\n",
    "    }\n",
    "    \n",
    "    DATA_WORKSPACE[keys[\"X_train\"]] = X_train\n",
    "    DATA_WORKSPACE[keys[\"X_test\"]] = X_test\n",
    "    DATA_WORKSPACE[keys[\"y_train\"]] = y_train\n",
    "    DATA_WORKSPACE[keys[\"y_test\"]] = y_test\n",
    "    \n",
    "    return {\"status\": \"success\", \"data_keys\": keys}\n",
    "    \n",
    "\n",
    "def apply_xgboost_classifier(X_train_key: str, y_train_key: str, n_estimators: Optional[int], max_depth: Optional[int], subsample: Optional[float], colsample_bytree: Optional[float]) -> dict:\n",
    "    \"\"\"\n",
    "    Trains an XGBoostClassifier model and stores it in the workspace.\n",
    "\n",
    "    Args:\n",
    "        X_train_key: The workspace key for the training features (X_train).\n",
    "        y_train_key: The workspace key for the training target (y_train).\n",
    "        n_estimators: Number of boosting rounds.\n",
    "        max_depth: Maximum tree depth.\n",
    "        subsample: Subsample ratio of the training instance.\n",
    "        colsample_bytree: Subsample ratio of columns when constructing each tree.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the key for the trained model.\n",
    "    \"\"\"\n",
    "    if X_train_key not in DATA_WORKSPACE or y_train_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Training data keys not found in workspace.\"}\n",
    "    \n",
    "    try:\n",
    "        X_train = DATA_WORKSPACE[X_train_key]\n",
    "        y_train = DATA_WORKSPACE[y_train_key]\n",
    "        \n",
    "        # Start with default hyperparameters\n",
    "        params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'enable_categorical': True\n",
    "        }\n",
    "        \n",
    "        # Create a dictionary of provided hyperparameters that are not None\n",
    "        provided_params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'subsample': subsample,\n",
    "            'colsample_bytree': colsample_bytree\n",
    "        }\n",
    "        \n",
    "        # Update defaults only with the values that were actually provided\n",
    "        # This filters out the 'None' values from the arguments\n",
    "        params.update({k: v for k, v in provided_params.items() if v is not None})\n",
    "\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        model_key = \"xgb_classifier_model\"\n",
    "        DATA_WORKSPACE[model_key] = model\n",
    "        \n",
    "        return {\"status\": \"success\", \"model_key\": model_key, \"hyperparameters_used\": params}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def apply_xgboost_regressor(X_train_key: str, y_train_key: str, n_estimators: Optional[int], max_depth: Optional[int], subsample: Optional[float], colsample_bytree: Optional[float]) -> dict:\n",
    "    \"\"\"\n",
    "    Trains an XGBoostRegressor model and stores it in the workspace.\n",
    "\n",
    "    Args:\n",
    "        X_train_key: The workspace key for the training features (X_train).\n",
    "        y_train_key: The workspace key for the training target (y_train).\n",
    "        n_estimators: Number of boosting rounds.\n",
    "        max_depth: Maximum tree depth.\n",
    "        subsample: Subsample ratio of the training instance.\n",
    "        colsample_bytree: Subsample ratio of columns when constructing each tree.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with status and the key for the trained model.\n",
    "    \"\"\"\n",
    "    if X_train_key not in DATA_WORKSPACE or y_train_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Training data keys not found in workspace.\"}\n",
    "\n",
    "    try:\n",
    "        X_train = DATA_WORKSPACE[X_train_key]\n",
    "        y_train = DATA_WORKSPACE[y_train_key]\n",
    "\n",
    "        # Start with default hyperparameters\n",
    "        params = {\n",
    "            'n_estimators': 100,\n",
    "            'max_depth': 3,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'enable_categorical': True\n",
    "        }\n",
    "\n",
    "        # Create a dictionary of provided hyperparameters that are not None\n",
    "        provided_params = {\n",
    "            'n_estimators': n_estimators,\n",
    "            'max_depth': max_depth,\n",
    "            'subsample': subsample,\n",
    "            'colsample_bytree': colsample_bytree\n",
    "        }\n",
    "\n",
    "        # Update defaults only with the values that were actually provided\n",
    "        params.update({k: v for k, v in provided_params.items() if v is not None})\n",
    "\n",
    "        model = XGBRegressor(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        model_key = \"xgb_regressor_model\"\n",
    "        DATA_WORKSPACE[model_key] = model\n",
    "        \n",
    "        return {\"status\": \"success\", \"model_key\": model_key, \"hyperparameters_used\": params}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "\n",
    "def evaluate_classifier_performance(model_key: str, X_test_key: str, y_test_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a classifier model using precision, recall, and F1-score.\n",
    "\n",
    "    Args:\n",
    "        model_key: The workspace key of the trained classifier model.\n",
    "        X_test_key: The workspace key of the test features (X_test).\n",
    "        y_test_key: The workspace key of the true test target values (y_test).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the status and performance metrics.\n",
    "    \"\"\"\n",
    "    if model_key not in DATA_WORKSPACE or X_test_key not in DATA_WORKSPACE or y_test_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Model or test data keys not found in workspace.\"}\n",
    "        \n",
    "    try:\n",
    "        model = DATA_WORKSPACE[model_key]\n",
    "        X_test = DATA_WORKSPACE[X_test_key]\n",
    "        y_test = DATA_WORKSPACE[y_test_key]\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        precision = precision_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "        recall = recall_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "        f1 = f1_score(y_test, y_pred, average='binary', zero_division=0)\n",
    "        \n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"metrics\": {\"Precision\": precision, \"Recall\": recall, \"F1-Score\": f1}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to evaluate classifier: {e}\"}\n",
    "\n",
    "\n",
    "def evaluate_regressor_performance(model_key: str, X_test_key: str, y_test_key: str) -> dict:\n",
    "    \"\"\"\n",
    "    Evaluates a regressor model using MAE, RMSE, and R-squared.\n",
    "\n",
    "    Args:\n",
    "        model_key: The workspace key of the trained regressor model.\n",
    "        X_test_key: The workspace key of the test features (X_test).\n",
    "        y_test_key: The workspace key of the true test target values (y_test).\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing the status and performance metrics.\n",
    "    \"\"\"\n",
    "    if model_key not in DATA_WORKSPACE or X_test_key not in DATA_WORKSPACE or y_test_key not in DATA_WORKSPACE:\n",
    "        return {\"status\": \"error\", \"message\": \"Model or test data keys not found in workspace.\"}\n",
    "\n",
    "    try:\n",
    "        model = DATA_WORKSPACE[model_key]\n",
    "        X_test = DATA_WORKSPACE[X_test_key]\n",
    "        y_test = DATA_WORKSPACE[y_test_key]\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "        return {\n",
    "            \"status\": \"success\",\n",
    "            \"metrics\": {\"MAE\": mae, \"RMSE\": rmse, \"R2\": r2}\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": f\"Failed to evaluate regressor: {e}\"}\n",
    "\n",
    "\n",
    "def exit_loop(tool_context: ToolContext) -> dict:\n",
    "    \"\"\"\n",
    "    Signals the main agent loop to stop iterating.\n",
    "\n",
    "    Args:\n",
    "        tool_context: The context object provided by the ADK framework.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary confirming that the exit signal has been sent.\n",
    "    \"\"\"\n",
    "    print(f\"--- [Tool Call] exit_loop activated by {tool_context.agent_name} ---\")\n",
    "    tool_context.actions.escalate = True\n",
    "    return {\"status\": \"success\", \"message\": \"Exit signal sent to the main loop.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bda8d20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lista das ferramentas disponíveis para o agente de Data Engineering\n",
    "ENGINEERING_TOOLS = [list_project_files,read_text_file, read_dataset, preview_dataset, dataset_info]\n",
    "\n",
    "# Lista das ferramentas disponíveis para o agente de Data Science\n",
    "SCIENCE_TOOLS = [read_dataset, preview_dataset, dataset_info, clean_dataset, convert_to_categorical,\n",
    "                 split_features_target, train_test_split_data, apply_xgboost_classifier, \n",
    "                 evaluate_classifier_performance, apply_xgboost_regressor, evaluate_regressor_performance]\n",
    "\n",
    "# Lista das ferramentas disponíveis para o agente de Avaliação\n",
    "CRITIQUE_TOOLS = [exit_loop]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81eb1a05",
   "metadata": {},
   "source": [
    "### Definição dos Agentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1b818ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. O Agente \"Engenheiro de Dados\"\n",
    "data_engineer_agent = LlmAgent(\n",
    "    name=\"DataEngineerAgent\",\n",
    "    model=GEMINI_MODEL,\n",
    "    instruction=f\"\"\"\n",
    "    You are a highly efficient Data Engineer AI. Your goal is to logically identify and prepare the features for a machine learning model. You must follow these steps in order:\n",
    "\n",
    "    **1. Inspect the Data Source:**\n",
    "       - Use the `list_project_files` tool to see the available files.\n",
    "       - Identify the primary data file (e.g., a `.csv`). \n",
    "       - Use the `read_dataset` tool in **inspection mode** (by providing an empty list to `use_columns`) on that file to get the exact list of available column names.\n",
    "\n",
    "    **2. Select Features:**\n",
    "       - The session state key '{STATE_CRITIQUE}' may contain feedback from a previous run. If it contains the signal \"{REENGINEER_SIGNAL}\", you **MUST** choose a **different combination of features** than before.\n",
    "       - Based *only* on the column list from the inspection step and the user's goal (`predict 'IN_INTERNET'`), select a maximum of 10 relevant columns. Include the target variable. **Do not guess column names.**\n",
    "\n",
    "    **3. Final Output:**\n",
    "       - Your final output for this turn **MUST** be a single, valid JSON object containing the keys `file_name`, `delimiter`, and `use_columns`.\n",
    "       - Note: Brazilian datasets often use a semicolon (';') as a delimiter.\n",
    "       - Example: {{\"file_name\": \"path/to/data.csv\", \"delimiter\": \";\", \"use_columns\": [\"col1\", \"col2\", \"col3\"]}}\n",
    "    \"\"\",\n",
    "    tools=ENGINEERING_TOOLS,\n",
    "    output_key=STATE_ENGINEERING_SUMMARY\n",
    ")\n",
    "\n",
    "# 2. O Agente \"Cientista de Dados\"\n",
    "data_scientist_agent = LlmAgent(\n",
    "    name=\"DataScientistAgent\",\n",
    "    model=GEMINI_MODEL,\n",
    "    instruction=f\"\"\"\n",
    "    You are a methodical Data Scientist AI. Your task is to preprocess data, train a model, and evaluate it. You must follow the tool chain logically.\n",
    "\n",
    "    **1. Load Data:**\n",
    "       - Get the data details (file name, columns) from the session state key '{STATE_ENGINEERING_SUMMARY}'.\n",
    "       - Call the `read_dataset` tool with these details. This tool returns a dictionary; use the `df_key` from its output in subsequent steps.\n",
    "\n",
    "    **2. Preprocess and Split:**\n",
    "       - Use the `clean_dataset` tool on the `df_key`.\n",
    "       - Use `convert_to_categorical` on the `df_key` for columns that are not numerical.\n",
    "       - Use `split_features_target` to separate features (X) and target (y).\n",
    "       - Use `train_test_split_data` on the resulting feature/target keys.\n",
    "\n",
    "    **3. Train Model:**\n",
    "       - The session state key '{STATE_HYPERPARAMETERS}' may contain specific hyperparameters.\n",
    "       - If it does, pass them as arguments to the `apply_xgboost_classifier` tool.\n",
    "       - If it does not, call the tool with its default parameters.\n",
    "\n",
    "    **4. Evaluate and Output:**\n",
    "       - Use the `evaluate_classifier_performance` tool on the trained model and test data.\n",
    "       - Your final output for this turn **MUST** be the complete dictionary returned by the evaluation tool.\n",
    "    \"\"\",\n",
    "    tools=SCIENCE_TOOLS,\n",
    "    output_key=STATE_PERFORMANCE_METRICS\n",
    ")\n",
    "\n",
    "# 3. O Agente \"Avaliador\"\n",
    "critique_agent = LlmAgent(\n",
    "    name=\"CritiqueAgent\",\n",
    "    model=GEMINI_MODEL,\n",
    "    instruction=f\"\"\"\n",
    "    You are a decisive Machine Learning Model Critic. Your role is to analyze model performance and determine the next action with structured output.\n",
    "\n",
    "    **1. Review Performance:**\n",
    "       - Analyze the performance dictionary from the session state key '{STATE_PERFORMANCE_METRICS}'.\n",
    "\n",
    "    **2. Make a Decision:**\n",
    "       - **If 'F1-Score' > 0.80:** The model is successful. Call the `exit_loop` tool to terminate the process.\n",
    "       - **If 0.40 < 'F1-Score' <= 0.80:** The features are likely adequate, but the model needs tuning. Propose new hyperparameters.\n",
    "       - **If 'F1-Score' <= 0.40:** The performance is very low, suggesting the features are poor. Signal a re-engineering of features.\n",
    "\n",
    "    **3. Format Your Output:**\n",
    "       - Your output **MUST** be a single, valid JSON object with a 'decision' key.\n",
    "       - If successful, you will have already called the `exit_loop` tool. As a fallback, output: {{\"decision\": \"STOP_SUCCESS\", \"reason\": \"Model performance is excellent.\"}}\n",
    "       - For tuning, output: {{\"decision\": \"TUNE_HYPERPARAMETERS\", \"hyperparameters\": {{\"n_estimators\": 150, \"max_depth\": 7}}}}\n",
    "       - For re-engineering, output: {{\"decision\": \"{REENGINEER_SIGNAL}\", \"reason\": \"Feature selection seems inadequate.\"}}\n",
    "    \"\"\",\n",
    "    tools=CRITIQUE_TOOLS,\n",
    "    output_key=STATE_CRITIQUE\n",
    ")\n",
    "\n",
    "# 4. O Agente \"Orquestrador\"\n",
    "main_pipeline_agent = LoopAgent(\n",
    "    name=\"MainPipelineAgent\",\n",
    "    sub_agents=[\n",
    "        data_engineer_agent,\n",
    "        data_scientist_agent,\n",
    "        critique_agent\n",
    "    ],\n",
    "    max_iterations=3 # Limite de loops\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6717cd",
   "metadata": {},
   "source": [
    "### Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce24a32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_pipeline():\n",
    "    \"\"\"Configures and runs the complete agent pipeline.\"\"\"\n",
    "    session_service = InMemorySessionService()\n",
    "    session = await session_service.create_session(app_name=APP_NAME, user_id=USER_ID, session_id=SESSION_ID)\n",
    "    runner = Runner(agent=main_pipeline_agent, app_name=APP_NAME, session_service=session_service)\n",
    "\n",
    "    print(f\"--- STARTING PIPELINE WITH QUERY ---\\n'{INITIAL_QUERY}'\\n\")\n",
    "    content = types.Content(role='user', parts=[types.Part(text=INITIAL_QUERY)])\n",
    "\n",
    "    # The final answer will be accumulated here\n",
    "    final_response = \"\"\n",
    "    \n",
    "    try:\n",
    "        async for event in runner.run_async(user_id=USER_ID, session_id=SESSION_ID, new_message=content):\n",
    "            # Skip empty events\n",
    "            if not event.content or not event.content.parts:\n",
    "                continue\n",
    "\n",
    "            processed_tool_part = False\n",
    "\n",
    "            for part in event.content.parts:\n",
    "                # 1. Check for a tool call (code the agent wants to run)\n",
    "                if hasattr(part, 'executable_code') and part.executable_code:\n",
    "                    print(f\"\\n>> {event.author} is calling a tool:\")\n",
    "                    print(\"```python\")\n",
    "                    print(part.executable_code.code)\n",
    "                    print(\"```\")\n",
    "                    processed_tool_part = True\n",
    "\n",
    "                # 2. Check for the result of a tool call\n",
    "                elif hasattr(part, 'code_execution_result') and part.code_execution_result:\n",
    "                    output_str = pprint.pformat(part.code_execution_result.output)\n",
    "                    print(f\"\\n>> Tool result for {event.author}:\")\n",
    "                    print(output_str)\n",
    "                    processed_tool_part = True\n",
    "\n",
    "            # 3. If we haven't processed a tool part, any text is likely a \"thought\" or the final answer\n",
    "            if not processed_tool_part:\n",
    "                for part in event.content.parts:\n",
    "                    # It now safely checks if part.text exists and is not None before trying to use it.\n",
    "                    if hasattr(part, 'text') and part.text:\n",
    "                        text_content = part.text.strip()\n",
    "                        if text_content: # Process only if there is actual text after stripping\n",
    "                            if event.author == main_pipeline_agent.name:\n",
    "                                 final_response += part.text\n",
    "                            else:\n",
    "                                print(f\"\\n>> {event.author} is thinking...\\n   {text_content}\")\n",
    "            \n",
    "            # Pause for TIME seconds after processing each event to respect rate limits.\n",
    "            await asyncio.sleep(TIME)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- AN ERROR OCCURRED ---\\n\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"\\n--- PIPELINE FINISHED ---\")\n",
    "    print(f\"\\nFinal Agent Response:\\n{final_response.strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aaf5116f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STARTING PIPELINE WITH QUERY ---\n",
      "'Verifique os dados contidos na pasta 'DATA' e encontre o arquivo principal referente às escolas. Utilize os dicionários de dados dos datasets (se existirem). O objetivo é prever se uma escola possui internet (`IN_INTERNET`). Selecione colunas relevantes (como a localização e infraestrutura da escola) para construir o modelo.Para o workflow, utilize somente as ferramentas previamente dadas.'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: Listing files in DATA ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   Okay, the file `DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv` seems to be the main data file. I will now inspect its columns.\n",
      "--- Tool: Inspected columns in DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   Okay, I have the column names. Based on the goal of predicting `IN_INTERNET`, here's a selection of potentially relevant features, focusing on location and infrastructure:\n",
      "\n",
      "*   `IN_INTERNET` (target variable)\n",
      "*   `TP_DEPENDENCIA` (Type of school - public, private etc.)\n",
      "*   `TP_LOCALIZACAO` (Location - urban, rural)\n",
      "*   `CO_REGIAO` (Region code)\n",
      "*   `CO_MUNICIPIO` (Municipality code)\n",
      "*   `IN_BANDA_LARGA` (Broadband access)\n",
      "*   `IN_LABORATORIO_INFORMATICA` (Has computer lab)\n",
      "*   `QT_EQUIP_MULTIMIDIA` (Number of multimedia equipment)\n",
      "*   `IN_EQUIP_IMPRESSORA` (Has printer)\n",
      "*   `IN_BIBLIOTECA` (Has library)\n",
      "--- Tool: read_dataset successful. Stored under key: df_microdados_ed_basica_2024 ---\n",
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   ```json\n",
      "{\"file_name\": \"DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv\", \"delimiter\": \";\", \"use_columns\": [\"IN_INTERNET\", \"TP_DEPENDENCIA\", \"TP_LOCALIZACAO\", \"CO_REGIAO\", \"CO_MUNICIPIO\", \"IN_BANDA_LARGA\", \"IN_LABORATORIO_INFORMATICA\", \"QT_EQUIP_MULTIMIDIA\", \"IN_EQUIP_IMPRESSORA\", \"IN_BIBLIOTECA\"]}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   I apologize for the error. It seems I missed that some hyperparameters are mandatory for the `apply_xgboost_classifier` tool. I will use the default values for these hyperparameters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   I apologize for the error. It seems there's an issue with the target variable `y`. The error message suggests that the unique values in `y_train` are interpreted as `[1.]` while `[0]` was expected. I will try to evaluate the model even with this error.\n",
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   I cannot proceed since the model was not trained successfully, and I do not have the model key. I will have to return an error.\n",
      "```json\n",
      "{\"evaluate_classifier_performance_response\": {\"message\": \"Model or test data keys not found in workspace.\", \"status\": \"error\"}}\n",
      "```\n",
      "\n",
      ">> CritiqueAgent is thinking...\n",
      "   ```json\n",
      "{\"decision\": \"REVISAR ENGENHARIA DE DADOS\", \"reason\": \"Feature selection seems inadequate.\"}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   OK. The previous feature selection was deemed inadequate. I will now select a different set of features, still aiming to predict `IN_INTERNET`. This time, I will focus more on student and teacher characteristics, in addition to some infrastructure aspects.\n",
      "\n",
      "*   `IN_INTERNET` (target variable)\n",
      "*   `TP_DEPENDENCIA` (Type of school)\n",
      "*   `TP_LOCALIZACAO` (Location - urban, rural)\n",
      "*   `QT_MAT_BAS` (Number of students in basic education)\n",
      "*   `QT_DOC_BAS` (Number of basic education teachers)\n",
      "*   `IN_LABORATORIO_INFORMATICA` (Has computer lab)\n",
      "*   `IN_BIBLIOTECA` (Has library)\n",
      "*   `IN_ALIMENTACAO` (Serves food)\n",
      "*   `IN_ENERGIA_REDE_PUBLICA` (Has public power grid)\n",
      "*   `IN_ESGOTO_REDE_PUBLICA` (Has public sewage system)\n",
      "--- Tool: read_dataset successful. Stored under key: df_microdados_ed_basica_2024 ---\n",
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   ```json\n",
      "{\"file_name\": \"DATA/microdados_censo_escolar_2024/dados/microdados_ed_basica_2024.csv\", \"delimiter\": \";\", \"use_columns\": [\"IN_INTERNET\", \"TP_DEPENDENCIA\", \"TP_LOCALIZACAO\", \"QT_MAT_BAS\", \"QT_DOC_BAS\", \"IN_LABORATORIO_INFORMATICA\", \"IN_BIBLIOTECA\", \"IN_ALIMENTACAO\", \"IN_ENERGIA_REDE_PUBLICA\", \"IN_ESGOTO_REDE_PUBLICA\"]}\n",
      "```\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n",
      "Warning: there are non-text parts in the response: ['function_call'], returning concatenated text result from text parts. Check the full candidates.content.parts accessor to get the full model response.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   ```json\n",
      "{\"evaluate_classifier_performance_response\": {\"metrics\": {\"F1-Score\": 0.97710773400445261, \"Precision\": 0.96243504790961532, \"Recall\": 0.99223472747825237}, \"status\": \"success\"}}\n",
      "```\n",
      "\n",
      ">> CritiqueAgent is thinking...\n",
      "   ```json\n",
      "{\"decision\": \"STOP_SUCCESS\", \"reason\": \"Model performance is excellent.\"}\n",
      "```\n",
      "\n",
      ">> DataEngineerAgent is thinking...\n",
      "   OK.\n",
      "\n",
      ">> DataScientistAgent is thinking...\n",
      "   ```json\n",
      "{\"evaluate_classifier_performance_response\": {\"metrics\": {\"F1-Score\": 0.97710773400445261, \"Precision\": 0.96243504790961532, \"Recall\": 0.99223472747825237}, \"status\": \"success\"}}\n",
      "```\n",
      "\n",
      ">> CritiqueAgent is thinking...\n",
      "   ```json\n",
      "{\"decision\": \"STOP_SUCCESS\", \"reason\": \"Model performance is excellent.\"}\n",
      "```\n",
      "\n",
      "--- PIPELINE FINISHED ---\n",
      "\n",
      "Final Agent Response:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Verifica se a variável API Key do Gemini NÃO existe\n",
    "    if not GOOGLE_API_KEY:\n",
    "        # Se não existir, avisa e ENCERRA o programa\n",
    "        print(\"ERRO: A variável de ambiente GOOGLE_API_KEY não foi encontrada.\")\n",
    "        exit() # Encerra o script aqui\n",
    "\n",
    "    # Inicializa a pipeline\n",
    "    # asyncio.run(run_pipeline())   # para arquivos .py\n",
    "    await run_pipeline()            # para arquivos .ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
